{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y58mbQ6ifWX1"
      },
      "source": [
        "# LSTM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Px1a9J80fWX2"
      },
      "source": [
        "Test of simple MLP models with different amount of neurons, hidden layers and size of input vector"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xdD6DEMtfWX3"
      },
      "source": [
        "## Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 174,
      "metadata": {
        "id": "FrRTGR5RfWX3"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "import torch.nn as nn\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "import plotly.express as px\n",
        "from torch.optim import Adam\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 175,
      "metadata": {
        "id": "oCmAM6xXfWX4"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 176,
      "metadata": {
        "id": "db8Npi-EfWX4"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"data_N_6000_noise_level_0.05.csv\")\n",
        "df_diff = pd.DataFrame(df.iloc[1:].values - df.iloc[:-1].values, columns=[\"dt\", \"dx\", \"dy\", \"dz\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 177,
      "metadata": {
        "id": "5BdrbXHvfWX4"
      },
      "outputs": [],
      "source": [
        "class SequentDataset(Dataset):\n",
        "    def __init__(self, dataframe: pd.DataFrame, n_dots=1, n_dot_parameters=4):\n",
        "        self.n_dot_parameters = n_dot_parameters\n",
        "        self.n_dots = n_dots\n",
        "        self.X_, self.y_ = self.__make_stack(dataframe)\n",
        "        print(self.X_.shape)\n",
        "        print(self.y_.shape)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X_)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X_[idx], self.y_[idx]\n",
        "\n",
        "    def __make_stack(self, df: pd.DataFrame):\n",
        "        seq_amount = df.shape[0] - 2 * self.n_dots - 1\n",
        "        X = torch.zeros((seq_amount, self.n_dots, self.n_dot_parameters), dtype=torch.float32)\n",
        "        y = torch.zeros((seq_amount, self.n_dots, self.n_dot_parameters), dtype=torch.float32)\n",
        "        for i in range(seq_amount):\n",
        "            X[i, :, :] = torch.reshape(torch.tensor(df.values[i:i+self.n_dots, :], dtype=torch.float32), (1, self.n_dots, self.n_dot_parameters))\n",
        "            y[i, :, :] = torch.reshape(torch.tensor(df.values[i+self.n_dots:i+2*self.n_dots, :], dtype=torch.float32), (1, self.n_dots, self.n_dot_parameters))\n",
        "\n",
        "        # stacks = [[df.iloc[:-self.n_dots]]] + [df.iloc[i:].values if (self.n_dots == i) else df.iloc[i:-(self.n_dots - i)].values for i in range(1, self.n_dots + 1)]\n",
        "        return (X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 178,
      "metadata": {
        "id": "I2kPC7WZfWX4"
      },
      "outputs": [],
      "source": [
        "N_DOTS = 20\n",
        "BATCH_SIZE = 8\n",
        "N_DOT_PARAMETERS = 4\n",
        "N_LSTM_LAYERS = N_DOTS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 179,
      "metadata": {
        "id": "TwxL3-AKfWX5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a7d747e-9a9b-46bb-e658-4590e4fd00f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([4959, 20, 4])\n",
            "torch.Size([4959, 20, 4])\n",
            "torch.Size([958, 20, 4])\n",
            "torch.Size([958, 20, 4])\n"
          ]
        }
      ],
      "source": [
        "dataset = SequentDataset(df_diff[0:5000], n_dots=N_DOTS, n_dot_parameters=N_DOT_PARAMETERS)\n",
        "test = SequentDataset(df_diff[5000:6000], n_dots=N_DOTS, n_dot_parameters=N_DOT_PARAMETERS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 180,
      "metadata": {
        "id": "M52g_bs-fWX5"
      },
      "outputs": [],
      "source": [
        "train_data, val_data = random_split(dataset,[0.8,0.2])\n",
        "\n",
        "train_loader = DataLoader(train_data, batch_size=N_DOTS, shuffle=True)\n",
        "val_loader = DataLoader(val_data, batch_size=N_DOTS, shuffle=False)\n",
        "test_loader = DataLoader(test, batch_size=N_DOTS, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AZiHwbfVfWX6"
      },
      "source": [
        "## Create Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 181,
      "metadata": {
        "id": "6nzrawHzfWX6"
      },
      "outputs": [],
      "source": [
        "class LSTMModel(nn.Module):\n",
        "    def __init__(self, input_size=4, hidden_layer_size=100, output_size=4, num_layers=10):\n",
        "        super(LSTMModel, self).__init__()\n",
        "        self.hidden_layer_size = hidden_layer_size\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        # Define the LSTM layer\n",
        "        self.lstm = nn.LSTM(input_size, hidden_layer_size, num_layers, batch_first=True)\n",
        "\n",
        "        # Define the output layer\n",
        "        self.linear = nn.Linear(hidden_layer_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.to(device)\n",
        "        batch_size = x.size(0)\n",
        "        self.h = torch.zeros(self.num_layers, batch_size, self.hidden_layer_size).requires_grad_()\n",
        "        self.h = self.h.to(device)\n",
        "        self.c = torch.zeros(self.num_layers, batch_size, self.hidden_layer_size).requires_grad_()\n",
        "        self.c = self.c.to(device)\n",
        "        out, _ = self.lstm(x, (self.h, self.c))\n",
        "\n",
        "        # print(out.size())\n",
        "\n",
        "        # Pass through fully connected layer\n",
        "        out = self.linear(out)  # We want the output corresponding to the last time step\n",
        "        return out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 182,
      "metadata": {
        "id": "vL6tWikkfWX6"
      },
      "outputs": [],
      "source": [
        "model = LSTMModel(input_size=N_DOT_PARAMETERS, num_layers=N_LSTM_LAYERS).to(device)\n",
        "loss_model = nn.MSELoss(reduction='mean')\n",
        "# loss_model = nn.L1Loss(reduction='mean')\n",
        "opt = Adam(model.parameters(), lr=0.001)\n",
        "lr_scheduler = torch.optim.lr_scheduler.StepLR(opt, step_size=5, gamma=0.95)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 183,
      "metadata": {
        "id": "sn6j-rj3fWX6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95007454-d74e-4fdd-aae5-78c49ce2464f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LSTMModel(\n",
              "  (lstm): LSTM(4, 100, num_layers=20, batch_first=True)\n",
              "  (linear): Linear(in_features=100, out_features=4, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 183
        }
      ],
      "source": [
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 184,
      "metadata": {
        "id": "O48vaiIjfWX7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "819e79e7-2edc-42e3-e7d4-c2f181a45f3b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/250], train_loss = 0.0144, val_loss = 0.0019\n",
            "Epoch [2/250], train_loss = 0.0019, val_loss = 0.0020\n",
            "Epoch [3/250], train_loss = 0.0019, val_loss = 0.0019\n",
            "Epoch [4/250], train_loss = 0.0019, val_loss = 0.0019\n",
            "Epoch [5/250], train_loss = 0.0019, val_loss = 0.0019\n",
            "Epoch [6/250], train_loss = 0.0019, val_loss = 0.0019\n",
            "Epoch [7/250], train_loss = 0.0019, val_loss = 0.0020\n",
            "Epoch [8/250], train_loss = 0.0019, val_loss = 0.0019\n",
            "Epoch [9/250], train_loss = 0.0019, val_loss = 0.0019\n",
            "Epoch [10/250], train_loss = 0.0019, val_loss = 0.0019\n",
            "Epoch [11/250], train_loss = 0.0019, val_loss = 0.0019\n",
            "Epoch [12/250], train_loss = 0.0019, val_loss = 0.0019\n",
            "Epoch [13/250], train_loss = 0.0019, val_loss = 0.0019\n",
            "Epoch [14/250], train_loss = 0.0019, val_loss = 0.0019\n",
            "Epoch [15/250], train_loss = 0.0019, val_loss = 0.0019\n",
            "Epoch [16/250], train_loss = 0.0019, val_loss = 0.0019\n",
            "Epoch [17/250], train_loss = 0.0019, val_loss = 0.0019\n",
            "Epoch [18/250], train_loss = 0.0019, val_loss = 0.0019\n",
            "Epoch [19/250], train_loss = 0.0019, val_loss = 0.0019\n",
            "Epoch [20/250], train_loss = 0.0019, val_loss = 0.0019\n",
            "Epoch [21/250], train_loss = 0.0019, val_loss = 0.0019\n",
            "Epoch [22/250], train_loss = 0.0019, val_loss = 0.0019\n",
            "Epoch [23/250], train_loss = 0.0019, val_loss = 0.0019\n",
            "Epoch [24/250], train_loss = 0.0019, val_loss = 0.0019\n",
            "Epoch [25/250], train_loss = 0.0019, val_loss = 0.0019\n",
            "Epoch [26/250], train_loss = 0.0019, val_loss = 0.0019\n",
            "Epoch [27/250], train_loss = 0.0019, val_loss = 0.0019\n",
            "Epoch [28/250], train_loss = 0.0019, val_loss = 0.0019\n",
            "Epoch [29/250], train_loss = 0.0019, val_loss = 0.0019\n",
            "Epoch [30/250], train_loss = 0.0019, val_loss = 0.0019\n",
            "Epoch [31/250], train_loss = 0.0019, val_loss = 0.0019\n",
            "Epoch [32/250], train_loss = 0.0019, val_loss = 0.0019\n",
            "Epoch [33/250], train_loss = 0.0019, val_loss = 0.0019\n",
            "Epoch [34/250], train_loss = 0.0019, val_loss = 0.0019\n",
            "Epoch [35/250], train_loss = 0.0019, val_loss = 0.0019\n",
            "Epoch [36/250], train_loss = 0.0019, val_loss = 0.0019\n",
            "Epoch [37/250], train_loss = 0.0019, val_loss = 0.0019\n",
            "Epoch [38/250], train_loss = 0.0019, val_loss = 0.0019\n",
            "Epoch [39/250], train_loss = 0.0019, val_loss = 0.0019\n",
            "Epoch [40/250], train_loss = 0.0019, val_loss = 0.0019\n",
            "Epoch [41/250], train_loss = 0.0019, val_loss = 0.0019\n",
            "Epoch [42/250], train_loss = 0.0019, val_loss = 0.0019\n",
            "Epoch [43/250], train_loss = 0.0019, val_loss = 0.0019\n",
            "Epoch [44/250], train_loss = 0.0019, val_loss = 0.0019\n",
            "Epoch [45/250], train_loss = 0.0019, val_loss = 0.0019\n",
            "Epoch [46/250], train_loss = 0.0019, val_loss = 0.0019\n",
            "Epoch [47/250], train_loss = 0.0019, val_loss = 0.0019\n",
            "Epoch [48/250], train_loss = 0.0019, val_loss = 0.0019\n",
            "Epoch [49/250], train_loss = 0.0019, val_loss = 0.0019\n",
            "Epoch [50/250], train_loss = 0.0019, val_loss = 0.0019\n",
            "Epoch [51/250], train_loss = 0.0019, val_loss = 0.0019\n",
            "Epoch [52/250], train_loss = 0.0019, val_loss = 0.0019\n",
            "Epoch [53/250], train_loss = 0.0019, val_loss = 0.0019\n",
            "Epoch [54/250], train_loss = 0.0019, val_loss = 0.0019\n",
            "Epoch [55/250], train_loss = 0.0019, val_loss = 0.0019\n",
            "Epoch [56/250], train_loss = 0.0019, val_loss = 0.0019\n",
            "Epoch [57/250], train_loss = 0.0019, val_loss = 0.0019\n",
            "Epoch [58/250], train_loss = 0.0019, val_loss = 0.0019\n",
            "Epoch [59/250], train_loss = 0.0019, val_loss = 0.0019\n",
            "Epoch [60/250], train_loss = 0.0019, val_loss = 0.0019\n",
            "Epoch [61/250], train_loss = 0.0019, val_loss = 0.0019\n",
            "Epoch [62/250], train_loss = 0.0019, val_loss = 0.0019\n",
            "Epoch [63/250], train_loss = 0.0019, val_loss = 0.0019\n",
            "Epoch [64/250], train_loss = 0.0019, val_loss = 0.0019\n",
            "Epoch [65/250], train_loss = 0.0019, val_loss = 0.0019\n",
            "Epoch [66/250], train_loss = 0.0019, val_loss = 0.0019\n",
            "Epoch [67/250], train_loss = 0.0019, val_loss = 0.0019\n",
            "Epoch [68/250], train_loss = 0.0019, val_loss = 0.0019\n",
            "Epoch [69/250], train_loss = 0.0019, val_loss = 0.0019\n",
            "Epoch [70/250], train_loss = 0.0019, val_loss = 0.0019\n",
            "Epoch [71/250], train_loss = 0.0019, val_loss = 0.0019\n",
            "Epoch [72/250], train_loss = 0.0019, val_loss = 0.0019\n",
            "Epoch [73/250], train_loss = 0.0019, val_loss = 0.0019\n",
            "Epoch [74/250], train_loss = 0.0019, val_loss = 0.0019\n",
            "Epoch [75/250], train_loss = 0.0019, val_loss = 0.0019\n",
            "Epoch [76/250], train_loss = 0.0019, val_loss = 0.0019\n",
            "Epoch [77/250], train_loss = 0.0019, val_loss = 0.0019\n",
            "Epoch [78/250], train_loss = 0.0019, val_loss = 0.0019\n",
            "Epoch [79/250], train_loss = 0.0019, val_loss = 0.0019\n",
            "Epoch [80/250], train_loss = 0.0019, val_loss = 0.0019\n",
            "Epoch [81/250], train_loss = 0.0019, val_loss = 0.0019\n",
            "Epoch [82/250], train_loss = 0.0019, val_loss = 0.0019\n",
            "Epoch [83/250], train_loss = 0.0019, val_loss = 0.0019\n",
            "Epoch [84/250], train_loss = 0.0019, val_loss = 0.0019\n",
            "Epoch [85/250], train_loss = 0.0019, val_loss = 0.0019\n",
            "Epoch [86/250], train_loss = 0.0019, val_loss = 0.0019\n",
            "Epoch [87/250], train_loss = 0.0019, val_loss = 0.0019\n",
            "Epoch [88/250], train_loss = 0.0019, val_loss = 0.0019\n",
            "Epoch [89/250], train_loss = 0.0019, val_loss = 0.0019\n",
            "Epoch [90/250], train_loss = 0.0019, val_loss = 0.0019\n",
            "Epoch [91/250], train_loss = 0.0019, val_loss = 0.0019\n",
            "Epoch [92/250], train_loss = 0.0019, val_loss = 0.0019\n",
            "Epoch [93/250], train_loss = 0.0019, val_loss = 0.0019\n",
            "Epoch [94/250], train_loss = 0.0019, val_loss = 0.0019\n",
            "Epoch [95/250], train_loss = 0.0019, val_loss = 0.0019\n",
            "Epoch [96/250], train_loss = 0.0019, val_loss = 0.0019\n",
            "Epoch [97/250], train_loss = 0.0019, val_loss = 0.0019\n",
            "Epoch [98/250], train_loss = 0.0019, val_loss = 0.0019\n",
            "Epoch [99/250], train_loss = 0.0019, val_loss = 0.0019\n",
            "Epoch [100/250], train_loss = 0.0019, val_loss = 0.0019\n",
            "Epoch [101/250], train_loss = 0.0019, val_loss = 0.0019\n",
            "Epoch [102/250], train_loss = 0.0019, val_loss = 0.0019\n",
            "Epoch [103/250], train_loss = 0.0019, val_loss = 0.0019\n",
            "Epoch [104/250], train_loss = 0.0019, val_loss = 0.0019\n",
            "Epoch [105/250], train_loss = 0.0019, val_loss = 0.0019\n",
            "Epoch [106/250], train_loss = 0.0019, val_loss = 0.0019\n",
            "Epoch [107/250], train_loss = 0.0019, val_loss = 0.0019\n",
            "Epoch [108/250], train_loss = 0.0019, val_loss = 0.0019\n",
            "Epoch [109/250], train_loss = 0.0019, val_loss = 0.0019\n",
            "Epoch [110/250], train_loss = 0.0019, val_loss = 0.0019\n",
            "Epoch [111/250], train_loss = 0.0019, val_loss = 0.0019\n",
            "Epoch [112/250], train_loss = 0.0019, val_loss = 0.0019\n",
            "Epoch [113/250], train_loss = 0.0019, val_loss = 0.0019\n",
            "Epoch [114/250], train_loss = 0.0019, val_loss = 0.0019\n",
            "Epoch [115/250], train_loss = 0.0019, val_loss = 0.0019\n",
            "Epoch [116/250], train_loss = 0.0019, val_loss = 0.0019\n",
            "Epoch [117/250], train_loss = 0.0019, val_loss = 0.0019\n",
            "Epoch [118/250], train_loss = 0.0019, val_loss = 0.0019\n",
            "Epoch [119/250], train_loss = 0.0019, val_loss = 0.0019\n",
            "Epoch [120/250], train_loss = 0.0019, val_loss = 0.0019\n",
            "Epoch [121/250], train_loss = 0.0019, val_loss = 0.0019\n",
            "Epoch [122/250], train_loss = 0.0019, val_loss = 0.0019\n",
            "Epoch [123/250], train_loss = 0.0019, val_loss = 0.0019\n",
            "Epoch [124/250], train_loss = 0.0019, val_loss = 0.0019\n",
            "Epoch [125/250], train_loss = 0.0019, val_loss = 0.0019\n",
            "Epoch [126/250], train_loss = 0.0019, val_loss = 0.0019\n",
            "Epoch [127/250], train_loss = 0.0019, val_loss = 0.0019\n",
            "Epoch [128/250], train_loss = 0.0019, val_loss = 0.0019\n",
            "Epoch [129/250], train_loss = 0.0019, val_loss = 0.0019\n",
            "Epoch [130/250], train_loss = 0.0019, val_loss = 0.0019\n",
            "Epoch [131/250], train_loss = 0.0019, val_loss = 0.0019\n",
            "Epoch [132/250], train_loss = 0.0019, val_loss = 0.0019\n",
            "Epoch [133/250], train_loss = 0.0019, val_loss = 0.0019\n",
            "Epoch [134/250], train_loss = 0.0019, val_loss = 0.0019\n",
            "Epoch [135/250], train_loss = 0.0019, val_loss = 0.0019\n",
            "Epoch [136/250], train_loss = 0.0019, val_loss = 0.0019\n",
            "Epoch [137/250], train_loss = 0.0019, val_loss = 0.0019\n",
            "Epoch [138/250], train_loss = 0.0019, val_loss = 0.0019\n",
            "Epoch [139/250], train_loss = 0.0019, val_loss = 0.0019\n",
            "Epoch [140/250], train_loss = 0.0019, val_loss = 0.0019\n",
            "Epoch [141/250], train_loss = 0.0019, val_loss = 0.0019\n",
            "Epoch [142/250], train_loss = 0.0019, val_loss = 0.0019\n",
            "Epoch [143/250], train_loss = 0.0019, val_loss = 0.0019\n",
            "Epoch [144/250], train_loss = 0.0019, val_loss = 0.0019\n",
            "Epoch [145/250], train_loss = 0.0019, val_loss = 0.0019\n",
            "Epoch [146/250], train_loss = 0.0019, val_loss = 0.0019\n",
            "Epoch [147/250], train_loss = 0.0019, val_loss = 0.0019\n",
            "Epoch [148/250], train_loss = 0.0019, val_loss = 0.0019\n",
            "Epoch [149/250], train_loss = 0.0019, val_loss = 0.0019\n",
            "Epoch [150/250], train_loss = 0.0019, val_loss = 0.0019\n",
            "Epoch [151/250], train_loss = 0.0019, val_loss = 0.0019\n",
            "Epoch [152/250], train_loss = 0.0019, val_loss = 0.0019\n",
            "Epoch [153/250], train_loss = 0.0019, val_loss = 0.0019\n",
            "Epoch [154/250], train_loss = 0.0019, val_loss = 0.0019\n",
            "Epoch [155/250], train_loss = 0.0019, val_loss = 0.0019\n",
            "Epoch [156/250], train_loss = 0.0019, val_loss = 0.0019\n",
            "Epoch [157/250], train_loss = 0.0019, val_loss = 0.0019\n",
            "Epoch [158/250], train_loss = 0.0019, val_loss = 0.0019\n",
            "Epoch [159/250], train_loss = 0.0019, val_loss = 0.0019\n",
            "Epoch [160/250], train_loss = 0.0019, val_loss = 0.0019\n",
            "Epoch [161/250], train_loss = 0.0019, val_loss = 0.0019\n",
            "Epoch [162/250], train_loss = 0.0019, val_loss = 0.0019\n",
            "Epoch [163/250], train_loss = 0.0019, val_loss = 0.0019\n",
            "Epoch [164/250], train_loss = 0.0019, val_loss = 0.0019\n",
            "Epoch [165/250], train_loss = 0.0019, val_loss = 0.0019\n",
            "Epoch [166/250], train_loss = 0.0019, val_loss = 0.0019\n",
            "Epoch [167/250], train_loss = 0.0019, val_loss = 0.0019\n",
            "Epoch [168/250], train_loss = 0.0019, val_loss = 0.0019\n",
            "Epoch [169/250], train_loss = 0.0019, val_loss = 0.0019\n",
            "Epoch [170/250], train_loss = 0.0019, val_loss = 0.0019\n",
            "Epoch [171/250], train_loss = 0.0019, val_loss = 0.0019\n",
            "Epoch [172/250], train_loss = 0.0019, val_loss = 0.0019\n",
            "Epoch [173/250], train_loss = 0.0019, val_loss = 0.0019\n",
            "Epoch [174/250], train_loss = 0.0019, val_loss = 0.0019\n",
            "Epoch [175/250], train_loss = 0.0019, val_loss = 0.0019\n",
            "Epoch [176/250], train_loss = 0.0019, val_loss = 0.0019\n",
            "Epoch [177/250], train_loss = 0.0019, val_loss = 0.0019\n",
            "Epoch [178/250], train_loss = 0.0019, val_loss = 0.0019\n",
            "Epoch [179/250], train_loss = 0.0019, val_loss = 0.0019\n",
            "Epoch [180/250], train_loss = 0.0019, val_loss = 0.0019\n",
            "Epoch [181/250], train_loss = 0.0019, val_loss = 0.0019\n",
            "Epoch [182/250], train_loss = 0.0019, val_loss = 0.0019\n",
            "Epoch [183/250], train_loss = 0.0019, val_loss = 0.0019\n",
            "Epoch [184/250], train_loss = 0.0019, val_loss = 0.0019\n",
            "Epoch [185/250], train_loss = 0.0019, val_loss = 0.0019\n",
            "Epoch [186/250], train_loss = 0.0019, val_loss = 0.0019\n",
            "Epoch [187/250], train_loss = 0.0019, val_loss = 0.0019\n",
            "Epoch [188/250], train_loss = 0.0019, val_loss = 0.0019\n",
            "Epoch [189/250], train_loss = 0.0019, val_loss = 0.0019\n",
            "Epoch [190/250], train_loss = 0.0019, val_loss = 0.0019\n",
            "Epoch [191/250], train_loss = 0.0019, val_loss = 0.0019\n",
            "Epoch [192/250], train_loss = 0.0019, val_loss = 0.0019\n",
            "Epoch [193/250], train_loss = 0.0019, val_loss = 0.0019\n",
            "Epoch [194/250], train_loss = 0.0019, val_loss = 0.0019\n",
            "Epoch [195/250], train_loss = 0.0019, val_loss = 0.0019\n",
            "Epoch [196/250], train_loss = 0.0019, val_loss = 0.0019\n",
            "Epoch [197/250], train_loss = 0.0019, val_loss = 0.0019\n",
            "Epoch [198/250], train_loss = 0.0019, val_loss = 0.0019\n",
            "Epoch [199/250], train_loss = 0.0019, val_loss = 0.0019\n",
            "Epoch [200/250], train_loss = 0.0019, val_loss = 0.0019\n",
            "Epoch [201/250], train_loss = 0.0019, val_loss = 0.0019\n",
            "Epoch [202/250], train_loss = 0.0019, val_loss = 0.0019\n",
            "Epoch [203/250], train_loss = 0.0019, val_loss = 0.0019\n",
            "Epoch [204/250], train_loss = 0.0019, val_loss = 0.0019\n",
            "Epoch [205/250], train_loss = 0.0019, val_loss = 0.0019\n",
            "Epoch [206/250], train_loss = 0.0019, val_loss = 0.0019\n",
            "Epoch [207/250], train_loss = 0.0019, val_loss = 0.0019\n",
            "Epoch [208/250], train_loss = 0.0019, val_loss = 0.0019\n",
            "Epoch [209/250], train_loss = 0.0019, val_loss = 0.0019\n",
            "Epoch [210/250], train_loss = 0.0019, val_loss = 0.0019\n",
            "Epoch [211/250], train_loss = 0.0019, val_loss = 0.0019\n",
            "Epoch [212/250], train_loss = 0.0019, val_loss = 0.0019\n",
            "Epoch [213/250], train_loss = 0.0019, val_loss = 0.0019\n",
            "Epoch [214/250], train_loss = 0.0019, val_loss = 0.0019\n",
            "Epoch [215/250], train_loss = 0.0019, val_loss = 0.0019\n",
            "Epoch [216/250], train_loss = 0.0019, val_loss = 0.0019\n",
            "Epoch [217/250], train_loss = 0.0019, val_loss = 0.0019\n",
            "Epoch [218/250], train_loss = 0.0019, val_loss = 0.0019\n",
            "Epoch [219/250], train_loss = 0.0019, val_loss = 0.0019\n",
            "Epoch [220/250], train_loss = 0.0019, val_loss = 0.0019\n",
            "Epoch [221/250], train_loss = 0.0019, val_loss = 0.0019\n",
            "Epoch [222/250], train_loss = 0.0019, val_loss = 0.0019\n",
            "Epoch [223/250], train_loss = 0.0019, val_loss = 0.0019\n",
            "Epoch [224/250], train_loss = 0.0019, val_loss = 0.0019\n",
            "Epoch [225/250], train_loss = 0.0019, val_loss = 0.0019\n",
            "Epoch [226/250], train_loss = 0.0019, val_loss = 0.0019\n",
            "Epoch [227/250], train_loss = 0.0019, val_loss = 0.0019\n",
            "Epoch [228/250], train_loss = 0.0019, val_loss = 0.0019\n",
            "Epoch [229/250], train_loss = 0.0019, val_loss = 0.0019\n",
            "Epoch [230/250], train_loss = 0.0019, val_loss = 0.0019\n",
            "Epoch [231/250], train_loss = 0.0019, val_loss = 0.0019\n",
            "Epoch [232/250], train_loss = 0.0019, val_loss = 0.0019\n",
            "Epoch [233/250], train_loss = 0.0019, val_loss = 0.0019\n",
            "Epoch [234/250], train_loss = 0.0019, val_loss = 0.0019\n",
            "Epoch [235/250], train_loss = 0.0019, val_loss = 0.0019\n",
            "Epoch [236/250], train_loss = 0.0019, val_loss = 0.0019\n",
            "Epoch [237/250], train_loss = 0.0019, val_loss = 0.0019\n",
            "Epoch [238/250], train_loss = 0.0019, val_loss = 0.0019\n",
            "Epoch [239/250], train_loss = 0.0019, val_loss = 0.0019\n",
            "Epoch [240/250], train_loss = 0.0019, val_loss = 0.0019\n",
            "Epoch [241/250], train_loss = 0.0019, val_loss = 0.0019\n",
            "Epoch [242/250], train_loss = 0.0019, val_loss = 0.0019\n",
            "Epoch [243/250], train_loss = 0.0019, val_loss = 0.0019\n",
            "Epoch [244/250], train_loss = 0.0019, val_loss = 0.0019\n",
            "Epoch [245/250], train_loss = 0.0019, val_loss = 0.0019\n",
            "Epoch [246/250], train_loss = 0.0019, val_loss = 0.0019\n",
            "Epoch [247/250], train_loss = 0.0019, val_loss = 0.0019\n",
            "Epoch [248/250], train_loss = 0.0019, val_loss = 0.0019\n",
            "Epoch [249/250], train_loss = 0.0019, val_loss = 0.0019\n",
            "Epoch [250/250], train_loss = 0.0019, val_loss = 0.0019\n"
          ]
        }
      ],
      "source": [
        "EPOCH = 250\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "\n",
        "for epoch in range(EPOCH):\n",
        "\n",
        "    # Обучение\n",
        "    model.train()\n",
        "    train_loss = []\n",
        "    for X, y in train_loader:\n",
        "        X = X.to(device)\n",
        "        y = y.to(device)\n",
        "\n",
        "        y_pred = model(X)\n",
        "        loss = loss_model(y_pred, y)\n",
        "        train_loss.append(loss.item())\n",
        "\n",
        "        opt.zero_grad()\n",
        "        loss.backward()\n",
        "\n",
        "        opt.step()\n",
        "        mean_train_loss = sum(train_loss)/len(train_loss)\n",
        "        # train_loop.set_description(f\"Epoch [{epoch+1}/{EPOCH}], train_loss = {mean_train_loss:.4f}\")\n",
        "\n",
        "    train_losses.append(mean_train_loss)\n",
        "\n",
        "    # Валидация\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        val_loss = []\n",
        "        for X, y in val_loader:\n",
        "            X = X.to(device)\n",
        "            y = y.to(device)\n",
        "            pred = model(X)\n",
        "            loss = loss_model(pred, y)\n",
        "            val_loss.append(loss.item())\n",
        "\n",
        "        mean_val_loss = sum(val_loss)/len(val_loss)\n",
        "        val_losses.append(mean_val_loss)\n",
        "\n",
        "    lr_scheduler.step()\n",
        "    lr = lr_scheduler.get_last_lr()\n",
        "    print(f\"Epoch [{epoch+1}/{EPOCH}], train_loss = {mean_train_loss:.4f}, val_loss = {mean_val_loss:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 185,
      "metadata": {
        "id": "T2V4qQH2fWX7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c4a4eaef-0c6d-48e3-d83c-b07df8d82486"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([11, 20, 4])"
            ]
          },
          "metadata": {},
          "execution_count": 185
        }
      ],
      "source": [
        "pred.size()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 186,
      "metadata": {
        "id": "Lvl9SiTVfWX7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "decb1766-9167-49f3-f82b-6b41ae01ce05"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1, 2, 3]"
            ]
          },
          "metadata": {},
          "execution_count": 186
        }
      ],
      "source": [
        "a = [1, 2, 3, 4]\n",
        "a[:-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 187,
      "metadata": {
        "id": "c44s778LfWX7"
      },
      "outputs": [],
      "source": [
        "start, _  = dataset[0]\n",
        "# print(start.size())\n",
        "# start = torch.reshape(start, (1, start.size(0)))\n",
        "predictions = []\n",
        "model.eval()\n",
        "\n",
        "for X, y in train_loader:\n",
        "    with torch.no_grad():\n",
        "        for i in range(N_DOTS, 500):\n",
        "            y_pred = model(X)\n",
        "            predictions.append(y_pred[0, -1, :])\n",
        "            X[0, :-1, :] = X.clone()[0, 1:, :]\n",
        "            X[0, -1, :] = y_pred[0, -1, :]\n",
        "\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 188,
      "metadata": {
        "id": "ch6fHHXAfWX7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6cd4384f-5e72-499a-8eb4-c6cd2c8a8ac4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 1.0337e+00,  6.7231e-02,  8.3389e-02,  7.3115e-02],\n",
              "        [ 9.6222e-01, -2.0753e-02, -7.6913e-02, -4.3385e-04],\n",
              "        [ 9.9967e-01,  1.3553e-02, -1.1785e-02, -4.0977e-02],\n",
              "        [ 1.0167e+00, -1.5033e-02,  5.9923e-03,  3.6317e-02],\n",
              "        [ 1.0725e+00, -3.3989e-02,  5.9333e-02,  1.5001e-02],\n",
              "        [ 9.8092e-01,  3.1740e-02, -7.1350e-02, -4.9139e-02],\n",
              "        [ 9.4035e-01, -1.7860e-03,  8.0157e-03,  3.4894e-02],\n",
              "        [ 1.0501e+00, -4.8864e-02, -1.8851e-02, -1.9254e-02],\n",
              "        [ 9.8786e-01,  1.5131e-02,  1.8590e-02, -2.7577e-02],\n",
              "        [ 1.0439e+00, -9.7870e-04, -2.6854e-02, -1.2646e-02],\n",
              "        [ 9.8502e-01, -2.3867e-02, -2.5049e-02, -3.2565e-02],\n",
              "        [ 9.6441e-01,  7.7511e-02,  1.4802e-02,  3.4769e-02],\n",
              "        [ 1.0205e+00, -8.9182e-02,  3.6208e-02, -4.5491e-02],\n",
              "        [ 9.4417e-01,  3.7399e-02, -4.7262e-02,  7.3084e-02],\n",
              "        [ 1.0558e+00, -6.4765e-03, -2.4762e-02, -8.9856e-02],\n",
              "        [ 1.0338e+00, -3.4726e-02,  3.1416e-02,  6.8764e-02],\n",
              "        [ 9.6135e-01,  5.2081e-02, -2.3326e-02, -1.2261e-02],\n",
              "        [ 9.9607e-01, -1.3776e-02, -1.8497e-02, -8.3296e-02],\n",
              "        [ 9.9546e-01, -4.0576e-02, -2.2712e-02,  5.5285e-02],\n",
              "        [ 9.6225e-01,  5.6257e-02, -1.4911e-02, -3.2622e-02]])"
            ]
          },
          "metadata": {},
          "execution_count": 188
        }
      ],
      "source": [
        "start"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 189,
      "metadata": {
        "id": "TgrP1h7bfWX7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "32c48b79-4352-47c8-e0a6-739267c30f61"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "480"
            ]
          },
          "metadata": {},
          "execution_count": 189
        }
      ],
      "source": [
        "len(predictions)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y.size()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eArYqf9h-rHL",
        "outputId": "17341b26-d3f0-41ea-f91f-6dbe923258e0"
      },
      "execution_count": 190,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([20, 20, 4])"
            ]
          },
          "metadata": {},
          "execution_count": 190
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 191,
      "metadata": {
        "id": "YslfK0BafWX8"
      },
      "outputs": [],
      "source": [
        "p_x = pd.DataFrame(np.vstack(list(map(lambda x: x.cpu().detach().numpy(), predictions))), columns=[\"Time\", \"X\", \"Y\", \"Z\"])\n",
        "_, y  = dataset[:500]\n",
        "p_y = pd.DataFrame(y.cpu().detach().numpy()[:, 0, :], columns=[\"Time\", \"X\", \"Y\", \"Z\"])\n",
        "\n",
        "p_x = p_x.cumsum()\n",
        "p_y = p_y.cumsum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 192,
      "metadata": {
        "id": "2L6PDYoVfWX8"
      },
      "outputs": [],
      "source": [
        "p_x[\"C\"] = \"Predict\"\n",
        "p_y[\"C\"] = \"True\"\n",
        "\n",
        "d = pd.concat([p_x,p_y],axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 193,
      "metadata": {
        "id": "Q9MHnuLVfWX8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "outputId": "f1d57386-92e6-40b4-814a-d07f63eb3786"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.35.2.min.js\"></script>                <div id=\"60ffe0a4-74d4-40cf-bfdb-da433579caa2\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"60ffe0a4-74d4-40cf-bfdb-da433579caa2\")) {                    Plotly.newPlot(                        \"60ffe0a4-74d4-40cf-bfdb-da433579caa2\",                        [{\"hovertemplate\":\"C=Predict\\u003cbr\\u003eX=%{x}\\u003cbr\\u003eY=%{y}\\u003cbr\\u003eZ=%{z}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"Predict\",\"line\":{\"color\":\"#636efa\",\"dash\":\"solid\"},\"marker\":{\"symbol\":\"circle\"},\"mode\":\"lines\",\"name\":\"Predict\",\"scene\":\"scene\",\"showlegend\":true,\"x\":[-0.0006690919399261475,-0.001338183879852295,-0.0020072758197784424,-0.00267636775970459,-0.0033454596996307373,-0.004014551639556885,-0.004683643579483032,-0.005352731794118881,-0.006021823734045029,-0.006690915673971176,-0.007360007613897324,-0.008029099553823471,-0.008698191493749619,-0.009367283433675766,-0.010036375373601913,-0.010705467313528061,-0.01137455552816391,-0.012043647468090057,-0.012712739408016205,-0.013381831347942352,-0.0140509232878685,-0.014720015227794647,-0.015389107167720795,-0.016058199107646942,-0.01672729104757309,-0.01739637926220894,-0.018065467476844788,-0.018734559416770935,-0.019403647631406784,-0.02007273957133293,-0.02074183151125908,-0.021410923451185226,-0.022080015391111374,-0.02274910733103752,-0.02341819927096367,-0.024087291210889816,-0.024756383150815964,-0.025425471365451813,-0.026094559580087662,-0.02676365152001381,-0.027432739734649658,-0.028101831674575806,-0.028770923614501953,-0.0294400155544281,-0.030109107494354248,-0.030778199434280396,-0.031447287648916245,-0.03211637958884239,-0.03278547152876854,-0.03345456346869469,-0.034123655408620834,-0.03479274734854698,-0.03546183928847313,-0.03613092750310898,-0.036800019443035126,-0.03746911138296127,-0.03813820332288742,-0.03880729526281357,-0.039476387202739716,-0.04014547914266586,-0.04081457108259201,-0.04148366302251816,-0.042152754962444305,-0.04282184690237045,-0.0434909388422966,-0.04416003078222275,-0.044829122722148895,-0.04549821466207504,-0.04616730287671089,-0.04683639481663704,-0.04750548675656319,-0.048174578696489334,-0.04884367063641548,-0.04951276257634163,-0.050181854516267776,-0.050850946456193924,-0.05152003839612007,-0.05218913033604622,-0.05285821855068207,-0.053527310490608215,-0.05419640243053436,-0.05486549437046051,-0.05553458631038666,-0.056203678250312805,-0.056872766464948654,-0.0575418584048748,-0.05821095034480095,-0.0588800385594368,-0.059549130499362946,-0.06021822243928909,-0.06088731437921524,-0.06155640631914139,-0.062225498259067535,-0.06289459019899368,-0.06356368213891983,-0.06423277407884598,-0.06490186601877213,-0.06557095795869827,-0.06624004989862442,-0.06690914183855057,-0.06757823377847672,-0.06824731826782227,-0.06891641020774841,-0.06958550214767456,-0.07025459408760071,-0.07092368602752686,-0.071592777967453,-0.07226186990737915,-0.0729309618473053,-0.07360005378723145,-0.07426914572715759,-0.07493823766708374,-0.07560732960700989,-0.07627642154693604,-0.07694551348686218,-0.07761460542678833,-0.07828369736671448,-0.07895278930664062,-0.07962188124656677,-0.08029097318649292,-0.08096006512641907,-0.08162915706634521,-0.08229824900627136,-0.08296734094619751,-0.08363643288612366,-0.0843055248260498,-0.08497461676597595,-0.0856437087059021,-0.08631280064582825,-0.0869818925857544,-0.08765098452568054,-0.08832007646560669,-0.08898916840553284,-0.08965826034545898,-0.09032735228538513,-0.09099644422531128,-0.09166552871465683,-0.09233462065458298,-0.09300371259450912,-0.09367280453443527,-0.09434188902378082,-0.09501098096370697,-0.09568007290363312,-0.09634916484355927,-0.09701825678348541,-0.09768734872341156,-0.09835644066333771,-0.09902553260326385,-0.09969462454319,-0.10036371648311615,-0.1010328084230423,-0.10170190036296844,-0.10237099230289459,-0.10304008424282074,-0.10370917618274689,-0.10437826812267303,-0.10504736006259918,-0.10571645200252533,-0.10638554394245148,-0.10705463588237762,-0.10772372782230377,-0.10839281976222992,-0.10906191170215607,-0.10973100364208221,-0.11040009558200836,-0.11106918752193451,-0.11173827946186066,-0.1124073714017868,-0.11307646334171295,-0.1137455552816391,-0.11441464722156525,-0.1150837391614914,-0.11575283110141754,-0.11642192304134369,-0.11709101498126984,-0.11776010692119598,-0.11842919886112213,-0.11909829080104828,-0.11976738274097443,-0.12043647468090057,-0.12110556662082672,-0.12177465856075287,-0.12244375050067902,-0.12311284244060516,-0.12378193438053131,-0.12445101886987686,-0.1251201033592224,-0.12578919529914856,-0.1264582872390747,-0.12712737917900085,-0.127796471118927,-0.12846556305885315,-0.1291346549987793,-0.12980374693870544,-0.1304728388786316,-0.13114193081855774,-0.1318110227584839,-0.13248011469841003,-0.13314920663833618,-0.13381829857826233,-0.13448739051818848,-0.13515648245811462,-0.13582557439804077,-0.13649466633796692,-0.13716375827789307,-0.1378328502178192,-0.13850194215774536,-0.1391710340976715,-0.13984012603759766,-0.1405092179775238,-0.14117830991744995,-0.1418474018573761,-0.14251649379730225,-0.1431855857372284,-0.14385467767715454,-0.1445237696170807,-0.14519286155700684,-0.14586195349693298,-0.14653104543685913,-0.14720013737678528,-0.14786922931671143,-0.14853832125663757,-0.14920741319656372,-0.14987650513648987,-0.15054559707641602,-0.15121468901634216,-0.1518837809562683,-0.15255287289619446,-0.1532219648361206,-0.15389105677604675,-0.1545601487159729,-0.15522924065589905,-0.1558983325958252,-0.15656742453575134,-0.1572365164756775,-0.15790560841560364,-0.15857470035552979,-0.15924379229545593,-0.15991288423538208,-0.16058197617530823,-0.16125106811523438,-0.16192016005516052,-0.16258925199508667,-0.16325834393501282,-0.16392743587493896,-0.1645965278148651,-0.16526561975479126,-0.1659347116947174,-0.16660380363464355,-0.1672728955745697,-0.16794198751449585,-0.168611079454422,-0.16928017139434814,-0.1699492633342743,-0.17061835527420044,-0.1712874472141266,-0.17195653915405273,-0.17262563109397888,-0.17329472303390503,-0.17396381497383118,-0.17463290691375732,-0.17530199885368347,-0.17597109079360962,-0.17664018273353577,-0.17730927467346191,-0.17797836661338806,-0.1786474585533142,-0.17931655049324036,-0.1799856424331665,-0.18065473437309265,-0.1813238263130188,-0.18199291825294495,-0.1826620101928711,-0.18333110213279724,-0.1840001940727234,-0.18466928601264954,-0.18533837795257568,-0.18600746989250183,-0.18667656183242798,-0.18734565377235413,-0.18801474571228027,-0.18868383765220642,-0.18935292959213257,-0.19002202153205872,-0.19069111347198486,-0.191360205411911,-0.19202929735183716,-0.1926983892917633,-0.19336748123168945,-0.1940365731716156,-0.19470566511154175,-0.1953747570514679,-0.19604384899139404,-0.1967129409313202,-0.19738203287124634,-0.19805112481117249,-0.19872021675109863,-0.19938930869102478,-0.20005840063095093,-0.20072749257087708,-0.20139658451080322,-0.20206567645072937,-0.20273476839065552,-0.20340386033058167,-0.2040729522705078,-0.20474204421043396,-0.2054111361503601,-0.20608022809028625,-0.2067493200302124,-0.20741841197013855,-0.2080875039100647,-0.20875659584999084,-0.209425687789917,-0.21009477972984314,-0.2107638716697693,-0.21143296360969543,-0.21210205554962158,-0.21277114748954773,-0.21344023942947388,-0.21410933136940002,-0.21477842330932617,-0.21544751524925232,-0.21611660718917847,-0.21678569912910461,-0.21745479106903076,-0.2181238830089569,-0.21879297494888306,-0.2194620668888092,-0.22013115882873535,-0.2208002507686615,-0.22146934270858765,-0.2221384346485138,-0.22280752658843994,-0.2234766185283661,-0.22414571046829224,-0.22481480240821838,-0.22548389434814453,-0.22615298628807068,-0.22682207822799683,-0.22749117016792297,-0.22816026210784912,-0.22882935404777527,-0.22949844598770142,-0.23016753792762756,-0.2308366298675537,-0.23150572180747986,-0.232174813747406,-0.23284390568733215,-0.2335129976272583,-0.23418208956718445,-0.2348511815071106,-0.23552027344703674,-0.2361893653869629,-0.23685845732688904,-0.23752754926681519,-0.23819664120674133,-0.23886573314666748,-0.23953482508659363,-0.24020391702651978,-0.24087300896644592,-0.24154210090637207,-0.24221119284629822,-0.24288028478622437,-0.2435493767261505,-0.24421846866607666,-0.2448875606060028,-0.24555665254592896,-0.2462257444858551,-0.24689483642578125,-0.2475639283657074,-0.24823302030563354,-0.2489021122455597,-0.24957120418548584,-0.250240296125412,-0.25090938806533813,-0.2515784800052643,-0.25224757194519043,-0.2529166638851166,-0.2535857558250427,-0.25425484776496887,-0.254923939704895,-0.25559303164482117,-0.2562621235847473,-0.25693121552467346,-0.2576003074645996,-0.25826939940452576,-0.2589384913444519,-0.25960758328437805,-0.2602766752243042,-0.26094576716423035,-0.2616148591041565,-0.26228395104408264,-0.2629530429840088,-0.26362213492393494,-0.2642912268638611,-0.26496031880378723,-0.2656294107437134,-0.2662985026836395,-0.2669675946235657,-0.2676366865634918,-0.26830577850341797,-0.2689748704433441,-0.26964396238327026,-0.2703130543231964,-0.27098214626312256,-0.2716512382030487,-0.27232033014297485,-0.272989422082901,-0.27365851402282715,-0.2743276059627533,-0.27499669790267944,-0.2756657898426056,-0.27633488178253174,-0.2770039737224579,-0.27767306566238403,-0.2783421576023102,-0.27901124954223633,-0.2796803414821625,-0.2803494334220886,-0.28101852536201477,-0.2816876173019409,-0.28235670924186707,-0.2830258011817932,-0.28369489312171936,-0.2843639850616455,-0.28503307700157166,-0.2857021689414978,-0.28637126088142395,-0.2870403528213501,-0.28770944476127625,-0.2883785367012024,-0.28904762864112854,-0.2897167205810547,-0.29038581252098083,-0.291054904460907,-0.29172399640083313,-0.2923930883407593,-0.2930621802806854,-0.2937312722206116,-0.2944003641605377,-0.29506945610046387,-0.29573854804039,-0.29640763998031616,-0.2970767319202423,-0.29774582386016846,-0.2984149158000946,-0.29908400774002075,-0.2997530996799469,-0.30042219161987305,-0.3010912835597992,-0.30176037549972534,-0.3024294674396515,-0.30309855937957764,-0.3037676513195038,-0.30443674325942993,-0.3051058351993561,-0.3057749271392822,-0.3064440190792084,-0.3071131110191345,-0.30778220295906067,-0.3084512948989868,-0.30912038683891296,-0.3097894787788391,-0.31045857071876526,-0.3111276626586914,-0.31179675459861755,-0.3124658465385437,-0.31313493847846985,-0.313804030418396,-0.31447312235832214,-0.3151422142982483,-0.31581130623817444,-0.3164803981781006,-0.31714949011802673,-0.3178185820579529,-0.31848767399787903,-0.3191567659378052,-0.3198258578777313,-0.32049494981765747,-0.3211640417575836],\"y\":[0.0016915053129196167,0.003383006900548935,0.005074508488178253,0.006766010075807571,0.008457515388727188,0.010149020701646805,0.011840522289276123,0.013532023876905441,0.01522352546453476,0.016915030777454376,0.018606532365083694,0.02029803767800331,0.021989542990922928,0.023681044578552246,0.025372549891471863,0.02706405520439148,0.028755560517311096,0.030447065830230713,0.03213856741786003,0.03383007273077965,0.035521574318408966,0.03721307963132858,0.0389045812189579,0.04059608280658722,0.04228758439421654,0.043979085981845856,0.045670587569475174,0.04736209288239479,0.04905359447002411,0.05074509605765343,0.052436601370573044,0.05412810668349266,0.05581961199641228,0.057511113584041595,0.059202615171670914,0.06089412048459053,0.06258562207221985,0.06427712738513947,0.06596863269805908,0.0676601380109787,0.06935164332389832,0.07104314863681793,0.07273465394973755,0.07442615926265717,0.07611766457557678,0.0778091698884964,0.07950067520141602,0.08119218051433563,0.08288368582725525,0.08457519114017487,0.08626669645309448,0.0879582017660141,0.08964970707893372,0.09134121239185333,0.09303271770477295,0.09472422301769257,0.09641572833061218,0.0981072336435318,0.09979873895645142,0.10149024426937103,0.10318174958229065,0.10487325489521027,0.10656476020812988,0.1082562655210495,0.10994777083396912,0.11163927614688873,0.11333078145980835,0.11502228677272797,0.11671379208564758,0.1184052973985672,0.12009680271148682,0.12178830802440643,0.12347981333732605,0.12517131865024567,0.12686282396316528,0.1285543292760849,0.13024583458900452,0.13193733990192413,0.13362884521484375,0.13532035052776337,0.13701185584068298,0.1387033611536026,0.14039486646652222,0.14208637177944183,0.14377787709236145,0.14546938240528107,0.14716088771820068,0.1488523930311203,0.15054389834403992,0.15223540365695953,0.15392690896987915,0.15561841428279877,0.15730991959571838,0.159001424908638,0.16069293022155762,0.16238443553447723,0.16407594084739685,0.16576744616031647,0.16745895147323608,0.1691504567861557,0.17084196209907532,0.17253346741199493,0.17422497272491455,0.17591647803783417,0.17760798335075378,0.1792994886636734,0.18099099397659302,0.18268249928951263,0.18437400460243225,0.18606550991535187,0.18775701522827148,0.1894485205411911,0.19114002585411072,0.19283153116703033,0.19452303647994995,0.19621454179286957,0.19790604710578918,0.1995975524187088,0.20128905773162842,0.20298056304454803,0.20467206835746765,0.20636357367038727,0.20805507898330688,0.2097465842962265,0.21143808960914612,0.21312959492206573,0.21482110023498535,0.21651260554790497,0.21820411086082458,0.2198956161737442,0.22158712148666382,0.22327862679958344,0.22497013211250305,0.22666163742542267,0.22835314273834229,0.2300446480512619,0.23173615336418152,0.23342765867710114,0.23511916399002075,0.23681066930294037,0.23850217461585999,0.2401936799287796,0.24188518524169922,0.24357669055461884,0.24526819586753845,0.24695970118045807,0.24865120649337769,0.2503427267074585,0.2520342171192169,0.25372570753097534,0.25541722774505615,0.25710874795913696,0.2588002383708954,0.2604917287826538,0.26218321919441223,0.26387470960617065,0.26556622982025146,0.2672577500343323,0.2689492702484131,0.2706407606601715,0.27233225107192993,0.27402374148368835,0.2757152318954468,0.2774067521095276,0.279098242521286,0.28078973293304443,0.28248125314712524,0.28417277336120605,0.28586429357528687,0.2875557839870453,0.2892472743988037,0.2909387946128845,0.29263028502464294,0.29432177543640137,0.2960132658481598,0.2977047562599182,0.299396276473999,0.30108779668807983,0.30277931690216064,0.30447083711624146,0.3061623275279999,0.3078538179397583,0.3095453083515167,0.31123679876327515,0.31292831897735596,0.31461983919143677,0.3163113296031952,0.3180028200149536,0.31969431042671204,0.32138580083847046,0.3230772912502289,0.3247687816619873,0.3264603018760681,0.32815179228782654,0.32984328269958496,0.3315347731113434,0.3332262635231018,0.3349177837371826,0.3366093039512634,0.33830079436302185,0.3399922847747803,0.3416838049888611,0.3433753252029419,0.3450668454170227,0.3467583656311035,0.3484498858451843,0.35014137625694275,0.35183286666870117,0.353524386882782,0.3552158772945404,0.35690736770629883,0.35859888792037964,0.36029040813446045,0.36198192834854126,0.3636734187602997,0.3653649091720581,0.36705639958381653,0.36874788999557495,0.37043941020965576,0.3721309006214142,0.3738223910331726,0.37551388144493103,0.37720537185668945,0.37889689207077026,0.3805883824825287,0.3822798728942871,0.3839713931083679,0.38566291332244873,0.38735443353652954,0.38904592394828796,0.3907374143600464,0.3924289047718048,0.39412039518356323,0.39581191539764404,0.39750343561172485,0.39919495582580566,0.4008864462375641,0.4025779366493225,0.40426942706108093,0.40596091747283936,0.4076524078845978,0.4093438982963562,0.411035418510437,0.4127269387245178,0.41441842913627625,0.41610991954803467,0.4178014099597931,0.4194929003715515,0.4211844205856323,0.42287594079971313,0.42456746101379395,0.42625898122787476,0.42795050144195557,0.429641991853714,0.4313334822654724,0.43302497267723083,0.43471646308898926,0.43640798330307007,0.4380994737148285,0.4397909641265869,0.4414824843406677,0.44317397475242615,0.44486546516418457,0.446556955575943,0.4482484459877014,0.44993993639945984,0.45163142681121826,0.4533229172229767,0.4550144076347351,0.45670589804649353,0.45839738845825195,0.4600888788700104,0.4617803692817688,0.4634718596935272,0.46516335010528564,0.46685487031936646,0.4685463607311249,0.4702378511428833,0.4719293415546417,0.47362083196640015,0.47531235218048096,0.47700387239456177,0.4786953926086426,0.480386883020401,0.4820783734321594,0.48376989364624023,0.48546141386032104,0.48715293407440186,0.4888444244861603,0.4905359148979187,0.4922274053096771,0.49391889572143555,0.49561041593551636,0.4973019063472748,0.4989933967590332,0.500684916973114,0.5023764371871948,0.5040679574012756,0.5057594776153564,0.5074509978294373,0.5091425180435181,0.5108340382575989,0.5125255584716797,0.5142170786857605,0.5159085988998413,0.5176001191139221,0.5192916393280029,0.5209831595420837,0.5226746797561646,0.5243661999702454,0.5260577201843262,0.527749240398407,0.5294407606124878,0.5311322808265686,0.5328238010406494,0.5345153212547302,0.536206841468811,0.5378983616828918,0.5395898818969727,0.5412814021110535,0.5429729223251343,0.5446644425392151,0.5463559627532959,0.5480474829673767,0.5497390031814575,0.5514305233955383,0.5531220436096191,0.5548135638237,0.5565050840377808,0.5581966042518616,0.5598881244659424,0.5615796446800232,0.563271164894104,0.5649626851081848,0.5666542053222656,0.5683457255363464,0.5700372457504272,0.5717287659645081,0.5734202861785889,0.5751118063926697,0.5768033266067505,0.5784948468208313,0.5801863670349121,0.5818778872489929,0.5835694074630737,0.5852609276771545,0.5869524478912354,0.5886439681053162,0.590335488319397,0.5920270085334778,0.5937185287475586,0.5954100489616394,0.5971015691757202,0.598793089389801,0.6004846096038818,0.6021761298179626,0.6038676500320435,0.6055591702461243,0.6072506904602051,0.6089422106742859,0.6106337308883667,0.6123252511024475,0.6140167713165283,0.6157082915306091,0.6173998117446899,0.6190913319587708,0.6207828521728516,0.6224743723869324,0.6241658926010132,0.625857412815094,0.6275489330291748,0.6292404532432556,0.6309319734573364,0.6326234936714172,0.634315013885498,0.6360065340995789,0.6376980543136597,0.6393895745277405,0.6410810947418213,0.6427726149559021,0.6444641351699829,0.6461556553840637,0.6478471755981445,0.6495386958122253,0.6512302160263062,0.652921736240387,0.6546132564544678,0.6563047766685486,0.6579962968826294,0.6596878170967102,0.661379337310791,0.6630708575248718,0.6647623777389526,0.6664538979530334,0.6681454181671143,0.6698369383811951,0.6715284585952759,0.6732199788093567,0.6749114990234375,0.6766030192375183,0.6782945394515991,0.6799860596656799,0.6816775798797607,0.6833691000938416,0.6850606203079224,0.6867521405220032,0.688443660736084,0.6901351809501648,0.6918267011642456,0.6935182213783264,0.6952097415924072,0.696901261806488,0.6985927820205688,0.7002843022346497,0.7019758224487305,0.7036673426628113,0.7053588628768921,0.7070503830909729,0.7087419033050537,0.7104334235191345,0.7121249437332153,0.7138164639472961,0.715507984161377,0.7171995043754578,0.7188910245895386,0.7205825448036194,0.7222740650177002,0.723965585231781,0.7256571054458618,0.7273486256599426,0.7290401458740234,0.7307316660881042,0.7324231863021851,0.7341147065162659,0.7358062267303467,0.7374977469444275,0.7391892671585083,0.7408807873725891,0.7425723075866699,0.7442638278007507,0.7459553480148315,0.7476468682289124,0.7493383884429932,0.751029908657074,0.7527214288711548,0.7544129490852356,0.7561044692993164,0.7577959895133972,0.759487509727478,0.7611790299415588,0.7628705501556396,0.7645620703697205,0.7662535905838013,0.7679451107978821,0.7696366310119629,0.7713281512260437,0.7730196714401245,0.7747111916542053,0.7764027118682861,0.7780942320823669,0.7797857522964478,0.7814772725105286,0.7831687927246094,0.7848603129386902,0.786551833152771,0.7882433533668518,0.7899348735809326,0.7916263937950134,0.7933179140090942,0.795009434223175,0.7967009544372559,0.7983924746513367,0.8000839948654175,0.8017755150794983,0.8034670352935791,0.8051585555076599,0.8068500757217407,0.8085415959358215,0.8102331161499023,0.8119246363639832],\"z\":[0.0007305964827537537,0.0014611929655075073,0.0021917857229709625,0.0029223784804344177,0.0036529749631881714,0.004383571445941925,0.00511416420340538,0.0058447569608688354,0.006575349718332291,0.007305946201086044,0.008036542683839798,0.008767139166593552,0.009497735649347305,0.01022832840681076,0.010958924889564514,0.011689521372318268,0.012420117855072021,0.013150714337825775,0.01388130709528923,0.014611903578042984,0.01534249633550644,0.016073092818260193,0.016803685575723648,0.017534278333187103,0.018264874815940857,0.01899547129869461,0.019726064056158066,0.02045666053891182,0.021187257021665573,0.02191784977912903,0.022648446261882782,0.023379042744636536,0.02410963922739029,0.024840231984853745,0.0255708247423172,0.026301421225070953,0.027032017707824707,0.027762610465288162,0.028493206948041916,0.02922380343079567,0.029954396188259125,0.03068498894572258,0.031415585428476334,0.03214617818593979,0.03287677466869354,0.033607367426157,0.03433796390891075,0.03506855666637421,0.03579914942383766,0.036529745906591415,0.03726034238934517,0.03799093887209892,0.038721535354852676,0.03945213183760643,0.040182728320360184,0.04091332480311394,0.04164392128586769,0.042374517768621445,0.0431051142513752,0.04383571073412895,0.044566307216882706,0.04529689997434616,0.046027496457099915,0.04675809293985367,0.04748868942260742,0.048219285905361176,0.04894988238811493,0.049680475145578384,0.05041107162833214,0.05114166811108589,0.051872264593839645,0.0526028610765934,0.053333453834056854,0.05406404659152031,0.05479464307427406,0.05552523955702782,0.05625583603978157,0.056986428797245026,0.05771702527999878,0.05844762176275253,0.05917821824550629,0.05990881472826004,0.060639407485723495,0.06137000024318695,0.062100593000650406,0.06283118575811386,0.06356178224086761,0.06429237127304077,0.06502296775579453,0.06575356423854828,0.06648416072130203,0.06721475720405579,0.06794535368680954,0.0686759501695633,0.06940654665231705,0.0701371431350708,0.07086773961782455,0.07159833610057831,0.07232892513275146,0.07305952161550522,0.07379011809825897,0.07452070713043213,0.07525129616260529,0.07598188519477844,0.0767124816775322,0.07744307816028595,0.0781736746430397,0.07890427112579346,0.07963486760854721,0.08036546409130096,0.08109605312347412,0.08182664960622787,0.08255724608898163,0.08328783512115479,0.08401843160390854,0.08474902808666229,0.08547962456941605,0.0862102210521698,0.08694081008434296,0.08767140656709671,0.08840200304985046,0.08913259953260422,0.08986319601535797,0.09059379249811172,0.09132438898086548,0.09205498546361923,0.09278558194637299,0.09351617097854614,0.0942467674612999,0.09497736394405365,0.0957079604268074,0.09643855690956116,0.09716914594173431,0.09789973497390747,0.09863032400608063,0.09936091303825378,0.10009150952100754,0.10082210600376129,0.10155270248651505,0.1022832989692688,0.10301388800144196,0.10374448448419571,0.10447508096694946,0.10520567744970322,0.10593627393245697,0.10666687041521072,0.10739746689796448,0.10812806338071823,0.10885865986347198,0.10958925634622574,0.11031985282897949,0.11105044931173325,0.111781045794487,0.11251163482666016,0.11324222385883331,0.11397282034158707,0.11470341682434082,0.11543401330709457,0.11616460978984833,0.11689520627260208,0.11762580275535583,0.11835639923810959,0.11908699572086334,0.1198175922036171,0.12054818868637085,0.1212787851691246,0.12200938165187836,0.12273997813463211,0.12347057461738586,0.12420116364955902,0.12493176013231277,0.12566235661506653,0.12639296054840088,0.12712356448173523,0.12785416841506958,0.12858475744724274,0.1293153464794159,0.13004595041275024,0.1307765543460846,0.13150715827941895,0.1322377473115921,0.13296833634376526,0.13369892537593842,0.13442951440811157,0.13516011834144592,0.13589072227478027,0.13662132620811462,0.13735191524028778,0.13808250427246094,0.1388131082057953,0.13954369723796844,0.1402742862701416,0.14100489020347595,0.1417354792356491,0.14246606826782227,0.14319665729999542,0.14392724633216858,0.14465785026550293,0.14538845419883728,0.14611904323101044,0.1468496322631836,0.14758023619651794,0.1483108401298523,0.14904144406318665,0.149772047996521,0.15050265192985535,0.1512332409620285,0.15196382999420166,0.152694433927536,0.15342502295970917,0.15415561199188232,0.15488621592521667,0.15561681985855103,0.15634742379188538,0.15707801282405853,0.1578086018562317,0.15853920578956604,0.1592697948217392,0.16000038385391235,0.1607309728860855,0.16146156191825867,0.16219216585159302,0.16292275488376617,0.16365334391593933,0.1643839329481125,0.16511452198028564,0.16584512591362,0.16657572984695435,0.1673063337802887,0.16803692281246185,0.168767511844635,0.16949810087680817,0.17022868990898132,0.17095929384231567,0.17168989777565002,0.17242050170898438,0.17315109074115753,0.1738816797733307,0.17461228370666504,0.1753428876399994,0.17607347667217255,0.1768040657043457,0.17753466963768005,0.1782652735710144,0.17899587750434875,0.1797264665365219,0.18045705556869507,0.18118765950202942,0.18191826343536377,0.18264886736869812,0.18337947130203247,0.18411007523536682,0.18484067916870117,0.18557128310203552,0.18630188703536987,0.18703247606754303,0.1877630650997162,0.18849366903305054,0.1892242580652237,0.18995484709739685,0.1906854510307312,0.19141604006290436,0.19214662909507751,0.19287721812725067,0.19360780715942383,0.19433839619159698,0.19506898522377014,0.1957995742559433,0.19653016328811646,0.1972607523202896,0.19799134135246277,0.19872193038463593,0.19945251941680908,0.20018310844898224,0.2009136974811554,0.20164430141448975,0.2023748904466629,0.20310547947883606,0.20383606851100922,0.20456665754318237,0.20529726147651672,0.20602786540985107,0.20675846934318542,0.20748905837535858,0.20821964740753174,0.2089502513408661,0.20968085527420044,0.2104114592075348,0.21114204823970795,0.2118726372718811,0.21260322630405426,0.21333381533622742,0.21406441926956177,0.21479500830173492,0.21552559733390808,0.21625620126724243,0.21698680520057678,0.21771740913391113,0.21844801306724548,0.21917860209941864,0.2199091911315918,0.22063979506492615,0.2213703840970993,0.22210097312927246,0.2228315770626068,0.22356218099594116,0.22429277002811432,0.22502335906028748,0.22575394809246063,0.2264845371246338,0.22721514105796814,0.2279457449913025,0.22867634892463684,0.2294069528579712,0.23013755679130554,0.2308681607246399,0.23159876465797424,0.2323293685913086,0.23305997252464294,0.2337905764579773,0.23452118039131165,0.235251784324646,0.23598238825798035,0.2367129772901535,0.23744356632232666,0.238174170255661,0.23890477418899536,0.23963536322116852,0.24036595225334167,0.24109654128551483,0.241827130317688,0.24255773425102234,0.2432883381843567,0.24401892721652985,0.244749516248703,0.24548012018203735,0.2462107241153717,0.24694131314754486,0.24767190217971802,0.24840249121189117,0.24913308024406433,0.2498636692762375,0.25059425830841064,0.251324862241745,0.25205546617507935,0.2527860701084137,0.25351667404174805,0.2542472779750824,0.25497788190841675,0.2557084858417511,0.25643908977508545,0.2571696937084198,0.25790029764175415,0.2586309015750885,0.25936150550842285,0.2600921094417572,0.26082271337509155,0.2615533173084259,0.26228392124176025,0.2630145251750946,0.26374512910842896,0.2644757330417633,0.26520633697509766,0.265936940908432,0.26666754484176636,0.2673981487751007,0.26812875270843506,0.2688593566417694,0.26958996057510376,0.2703205645084381,0.27105116844177246,0.2717817723751068,0.27251237630844116,0.2732429802417755,0.27397358417510986,0.2747041881084442,0.27543479204177856,0.2761653959751129,0.27689599990844727,0.2776266038417816,0.27835720777511597,0.2790878117084503,0.27981841564178467,0.280549019575119,0.28127962350845337,0.2820102274417877,0.28274083137512207,0.2834714353084564,0.28420203924179077,0.2849326431751251,0.2856632471084595,0.2863938510417938,0.2871244549751282,0.2878550589084625,0.2885856628417969,0.2893162667751312,0.2900468707084656,0.2907774746417999,0.2915080785751343,0.29223868250846863,0.292969286441803,0.29369989037513733,0.2944304943084717,0.29516109824180603,0.2958917021751404,0.29662230610847473,0.2973529100418091,0.29808351397514343,0.2988141179084778,0.29954472184181213,0.3002753257751465,0.30100592970848083,0.3017365336418152,0.30246713757514954,0.3031977415084839,0.30392834544181824,0.3046589493751526,0.30538955330848694,0.3061201572418213,0.30685076117515564,0.30758136510849,0.30831196904182434,0.3090425729751587,0.30977317690849304,0.3105037808418274,0.31123438477516174,0.3119649887084961,0.31269559264183044,0.3134261965751648,0.31415680050849915,0.3148874044418335,0.31561800837516785,0.3163486123085022,0.31707921624183655,0.3178098201751709,0.31854042410850525,0.3192710280418396,0.32000163197517395,0.3207322359085083,0.32146283984184265,0.322193443775177,0.32292404770851135,0.3236546516418457,0.32438525557518005,0.3251158595085144,0.32584646344184875,0.3265770673751831,0.32730767130851746,0.3280382752418518,0.32876887917518616,0.3294994831085205,0.33023008704185486,0.3309606909751892,0.33169129490852356,0.3324218988418579,0.33315250277519226,0.3338831067085266,0.33461371064186096,0.3353443145751953,0.33607491850852966,0.336805522441864,0.33753612637519836,0.3382667303085327,0.33899733424186707,0.3397279381752014,0.34045854210853577,0.3411891460418701,0.34191974997520447,0.3426503539085388,0.34338095784187317,0.3441115617752075,0.34484216570854187,0.3455727696418762,0.34630337357521057,0.3470339775085449,0.3477645814418793,0.3484951853752136,0.349225789308548,0.3499563932418823,0.3506869971752167],\"type\":\"scatter3d\"},{\"hovertemplate\":\"C=True\\u003cbr\\u003eX=%{x}\\u003cbr\\u003eY=%{y}\\u003cbr\\u003eZ=%{z}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"True\",\"line\":{\"color\":\"#EF553B\",\"dash\":\"solid\"},\"marker\":{\"symbol\":\"circle\"},\"mode\":\"lines\",\"name\":\"True\",\"scene\":\"scene\",\"showlegend\":true,\"x\":[-0.041827037930488586,-0.03190379589796066,-0.10710526257753372,-0.026959672570228577,-0.0492556169629097,-0.09987378865480423,-0.11473675072193146,-0.11629201471805573,-0.11646005511283875,-0.15034210681915283,-0.09896819293498993,-0.1344463974237442,-0.12402340024709702,-0.1356593668460846,-0.19414237141609192,-0.2047901451587677,-0.14810964465141296,-0.20726467669010162,-0.17220117151737213,-0.21839450299739838,-0.19224028289318085,-0.2601247727870941,-0.28931117057800293,-0.27990835905075073,-0.24786162376403809,-0.2752842903137207,-0.2857501804828644,-0.3295627534389496,-0.2903915345668793,-0.38308027386665344,-0.3369320333003998,-0.4014907777309418,-0.383500874042511,-0.4111652374267578,-0.43993622064590454,-0.4277358651161194,-0.43896484375,-0.3920934200286865,-0.4789632260799408,-0.4941290020942688,-0.4937484562397003,-0.4236298203468323,-0.4878321588039398,-0.5020043253898621,-0.5313122868537903,-0.4919494092464447,-0.5084974765777588,-0.5671124458312988,-0.4872944951057434,-0.5176990628242493,-0.5945157408714294,-0.5811757445335388,-0.5247210264205933,-0.5987856984138489,-0.612089991569519,-0.5870881080627441,-0.6229567527770996,-0.570692777633667,-0.6278616189956665,-0.5872159004211426,-0.6577612161636353,-0.5742760300636292,-0.587522566318512,-0.6400274634361267,-0.5928006768226624,-0.6547006368637085,-0.6372525095939636,-0.6602953672409058,-0.6363096237182617,-0.6305546760559082,-0.6960709691047668,-0.6877403259277344,-0.6886023283004761,-0.6487820148468018,-0.6394019722938538,-0.6891516447067261,-0.6443057656288147,-0.6290620565414429,-0.5974017381668091,-0.6371174454689026,-0.6640623807907104,-0.6485891342163086,-0.6587842106819153,-0.6409704685211182,-0.6644124388694763,-0.6557698845863342,-0.5732464790344238,-0.6129763126373291,-0.5962485074996948,-0.5667104125022888,-0.5898436307907104,-0.5471368432044983,-0.568348228931427,-0.5807652473449707,-0.5835543870925903,-0.5248222947120667,-0.511858344078064,-0.499335378408432,-0.4963931441307068,-0.513009250164032,-0.5324946045875549,-0.538901150226593,-0.4578142762184143,-0.5071662664413452,-0.486472487449646,-0.44931331276893616,-0.46933069825172424,-0.46640580892562866,-0.3865939974784851,-0.38319674134254456,-0.36100155115127563,-0.394939124584198,-0.4060016870498657,-0.3802796006202698,-0.3898240029811859,-0.3634587526321411,-0.33705615997314453,-0.27784866094589233,-0.3009285628795624,-0.2647233307361603,-0.29132959246635437,-0.23595857620239258,-0.2596089839935303,-0.19229158759117126,-0.20796862244606018,-0.22492019832134247,-0.2284698784351349,-0.21378979086875916,-0.12375925481319427,-0.17215761542320251,-0.17563867568969727,-0.1092647835612297,-0.126390278339386,-0.10843126475811005,-0.08736058324575424,-0.03147531673312187,-0.10576878488063812,-0.022631041705608368,-0.015514997765421867,0.024012064561247826,-0.054836705327034,0.019116319715976715,0.03371341899037361,0.02265002578496933,0.06416238844394684,0.0699639767408371,0.0335349403321743,0.07863759249448776,0.09292829036712646,0.13343499600887299,0.07492499053478241,0.0859329029917717,0.0866515263915062,0.10030258446931839,0.12627199292182922,0.1876276135444641,0.1765958070755005,0.19130313396453857,0.1539282202720642,0.22219440340995789,0.22685840725898743,0.18277204036712646,0.19703826308250427,0.2344663143157959,0.18437235057353973,0.19690175354480743,0.2699618637561798,0.22142678499221802,0.2018381506204605,0.2567829191684723,0.22110633552074432,0.24881456792354584,0.30668386816978455,0.3060370087623596,0.26211512088775635,0.22917290031909943,0.3219081163406372,0.3222440779209137,0.2831226587295532,0.3243826627731323,0.26666581630706787,0.3304292559623718,0.2384740114212036,0.29778990149497986,0.27361515164375305,0.28556278347969055,0.25442951917648315,0.24589724838733673,0.2354787141084671,0.23359251022338867,0.2757800221443176,0.23407211899757385,0.2649405002593994,0.31756436824798584,0.25578489899635315,0.2817895710468292,0.3032360076904297,0.22404862940311432,0.2545400559902191,0.2128070592880249,0.20039698481559753,0.2533986568450928,0.21365489065647125,0.25313326716423035,0.24822862446308136,0.20669054985046387,0.1635654717683792,0.24635346233844757,0.21353797614574432,0.18784786760807037,0.20219069719314575,0.22042661905288696,0.17037498950958252,0.15978701412677765,0.14194124937057495,0.12588351964950562,0.13142962753772736,0.12326610088348389,0.14523330330848694,0.15714791417121887,0.0674578920006752,0.07794938236474991,0.08050371706485748,0.06271535158157349,0.09560954570770264,0.03961816802620888,0.07705022394657135,0.05197218060493469,0.03681308031082153,0.023036595433950424,0.0795937329530716,0.06354223191738129,0.036172281950712204,0.012973671779036522,-0.01058150827884674,0.028710942715406418,0.020511627197265625,-0.02809569612145424,-0.00779399648308754,-0.07162073254585266,-0.044775765389204025,-0.004355169832706451,-0.06230921298265457,-0.021727830171585083,-0.04403652250766754,-0.07789459824562073,-0.07015269249677658,-0.05017406493425369,-0.103030264377594,-0.10049377381801605,-0.10146831721067429,-0.12177987396717072,-0.10663140565156937,-0.15931791067123413,-0.12618818879127502,-0.16471168398857117,-0.1355571299791336,-0.146109476685524,-0.12439215183258057,-0.09455932676792145,-0.09172020852565765,-0.14893749356269836,-0.135237455368042,-0.14165368676185608,-0.1619836986064911,-0.12791383266448975,-0.11888127028942108,-0.17220522463321686,-0.17277583479881287,-0.1557895541191101,-0.10414374619722366,-0.17716652154922485,-0.1630658060312271,-0.11057549715042114,-0.11545954644680023,-0.14918102324008942,-0.10168810188770294,-0.10360783338546753,-0.13756334781646729,-0.10882045328617096,-0.17829829454421997,-0.14679642021656036,-0.12554042041301727,-0.11807812750339508,-0.09577244520187378,-0.137288898229599,-0.1272907257080078,-0.10879918187856674,-0.1277075558900833,-0.13810159265995026,-0.05786865949630737,-0.10610324144363403,-0.08759415149688721,-0.031353551894426346,-0.08753447234630585,-0.10480577498674393,-0.04113566875457764,-0.05683879554271698,-0.038169827312231064,-0.06534357368946075,-0.05698429420590401,-0.020932286977767944,-0.033116232603788376,-0.050786327570676804,-0.04139313846826553,0.057131461799144745,0.022965319454669952,0.003493327647447586,0.02687131240963936,0.0710514634847641,0.03480583429336548,0.11385264992713928,0.06809225678443909,0.0723390281200409,0.0829194188117981,0.1427542269229889,0.074460469186306,0.13324324786663055,0.1429879069328308,0.10872948169708252,0.10957785695791245,0.11457081884145737,0.12671329081058502,0.20926213264465332,0.14501437544822693,0.2416996955871582,0.206229567527771,0.1750272959470749,0.1955702304840088,0.22072595357894897,0.24489009380340576,0.2832329571247101,0.23111431300640106,0.2606740891933441,0.2478708028793335,0.3456801474094391,0.33192747831344604,0.27872273325920105,0.28744301199913025,0.3235830068588257,0.3826484680175781,0.34527868032455444,0.37165331840515137,0.35240355134010315,0.36892297863960266,0.34607169032096863,0.3640091121196747,0.39656010270118713,0.4388870596885681,0.3941025733947754,0.41840681433677673,0.4802854061126709,0.42851775884628296,0.4387471079826355,0.46206673979759216,0.49813950061798096,0.47410890460014343,0.49663978815078735,0.4873349368572235,0.5275290608406067,0.4615597128868103,0.48055338859558105,0.47344067692756653,0.4682055711746216,0.5607537031173706,0.49026939272880554,0.5680603981018066,0.48080694675445557,0.5694661736488342,0.5460739731788635,0.5048145651817322,0.511543869972229,0.5926041007041931,0.5011794567108154,0.5402321815490723,0.5762543678283691,0.5910505652427673,0.5396064519882202,0.5158212184906006,0.5103529095649719,0.5744931101799011,0.5377389788627625,0.5785365700721741,0.5995197296142578,0.5380605459213257,0.5450008511543274,0.5717149972915649,0.6104129552841187,0.5416610836982727,0.5142248272895813,0.6095985770225525,0.539454460144043,0.5590394735336304,0.5978378057479858,0.59597247838974,0.5114060640335083,0.5893142819404602,0.533922553062439,0.5208949446678162,0.5310465693473816,0.5122615694999695,0.545081377029419,0.5190231204032898,0.5045382976531982,0.54233717918396,0.5307657718658447,0.48768073320388794,0.4859521985054016,0.4847944974899292,0.5147944688796997,0.5189883708953857,0.5373439192771912,0.4851088523864746,0.5232399702072144,0.5233821868896484,0.4982320964336395,0.5277470350265503,0.49711135029792786,0.5076532363891602,0.4902894198894501,0.48087164759635925,0.4910518527030945,0.42742210626602173,0.490725040435791,0.42718324065208435,0.4908159673213959,0.42244604229927063,0.42576542496681213,0.4155251383781433,0.4363258481025696,0.39188718795776367,0.46774744987487793,0.46623706817626953,0.4544112980365753,0.4707339107990265,0.41085511445999146,0.39085593819618225,0.39861035346984863,0.3889950215816498,0.3999108672142029,0.35590583086013794,0.4164539873600006,0.43570876121520996,0.363353967666626,0.3613109290599823,0.4086168706417084,0.4342421591281891,0.4164104759693146,0.40069082379341125,0.35987818241119385,0.36553823947906494,0.3952291011810303,0.365659236907959,0.4027884006500244,0.38915300369262695,0.32955002784729004,0.4034711718559265,0.40692606568336487,0.34636548161506653,0.36296504735946655,0.3942631483078003,0.3316858410835266,0.36253178119659424,0.33973628282546997,0.3893931210041046,0.3721435070037842,0.3916330635547638,0.33815842866897583,0.36497077345848083,0.33631831407546997,0.388212114572525,0.39110198616981506,0.42461448907852173,0.33770596981048584,0.34831130504608154,0.39702361822128296,0.3843543529510498,0.363164484500885,0.3867221176624298,0.3941962420940399,0.34544187784194946,0.42078670859336853,0.3584311008453369,0.4092928171157837,0.40637123584747314,0.4345839023590088,0.433244526386261,0.415412962436676,0.3846655488014221,0.4553563594818115,0.44931989908218384,0.39723914861679077,0.45039331912994385,0.45205625891685486,0.40587088465690613,0.4578849971294403,0.4698146879673004,0.4881212115287781,0.4396710991859436,0.43300580978393555],\"y\":[0.03080880083143711,-0.034318707883358,-0.002029549330472946,-0.09379519522190094,-0.1253771185874939,-0.09434087574481964,-0.09828599542379379,-0.12764126062393188,-0.18226218223571777,-0.21078455448150635,-0.1474878340959549,-0.18699736893177032,-0.1948273777961731,-0.3109934628009796,-0.27485716342926025,-0.33582964539527893,-0.3731798529624939,-0.38018277287483215,-0.39642295241355896,-0.4437859058380127,-0.42669767141342163,-0.4067281484603882,-0.4765865206718445,-0.5433503985404968,-0.522405207157135,-0.6057857871055603,-0.6086776256561279,-0.6330358982086182,-0.6197654604911804,-0.653202474117279,-0.7044097185134888,-0.7220700979232788,-0.7651301622390747,-0.7134788036346436,-0.7810616493225098,-0.8063666224479675,-0.8026323318481445,-0.829351544380188,-0.9377721548080444,-0.926525890827179,-0.9406208992004395,-0.9212597608566284,-1.0336225032806396,-1.010817289352417,-1.0705636739730835,-1.0894473791122437,-1.0894540548324585,-1.1047451496124268,-1.125109076499939,-1.1102309226989746,-1.1806563138961792,-1.2380433082580566,-1.2495107650756836,-1.2211822271347046,-1.273497223854065,-1.2697153091430664,-1.239345908164978,-1.2877753973007202,-1.3610432147979736,-1.3795735836029053,-1.3398369550704956,-1.382391333580017,-1.3472929000854492,-1.4313318729400635,-1.3937698602676392,-1.3935565948486328,-1.41306734085083,-1.408225178718567,-1.4158674478530884,-1.4670337438583374,-1.4369741678237915,-1.4680746793746948,-1.526800513267517,-1.5221810340881348,-1.5422879457473755,-1.5501232147216797,-1.522120714187622,-1.483076572418213,-1.5576587915420532,-1.4779493808746338,-1.4835890531539917,-1.497172236442566,-1.534985899925232,-1.5302351713180542,-1.5150398015975952,-1.523765206336975,-1.5312869548797607,-1.5040725469589233,-1.4553942680358887,-1.490799069404602,-1.4830691814422607,-1.4687317609786987,-1.469162940979004,-1.4405440092086792,-1.396641731262207,-1.3966885805130005,-1.4478455781936646,-1.4030823707580566,-1.3682559728622437,-1.4233604669570923,-1.4168778657913208,-1.3086028099060059,-1.3729486465454102,-1.3559085130691528,-1.3059030771255493,-1.2470866441726685,-1.2925409078598022,-1.224737524986267,-1.244125485420227,-1.220794916152954,-1.1876722574234009,-1.2139701843261719,-1.1491097211837769,-1.0851929187774658,-1.1435900926589966,-1.057049036026001,-1.0461890697479248,-1.0044296979904175,-1.0581738948822021,-1.0427354574203491,-1.0102957487106323,-0.9282052516937256,-0.9596142768859863,-0.8543643951416016,-0.9066060185432434,-0.870448112487793,-0.8341795206069946,-0.8182314038276672,-0.7542468905448914,-0.7830766439437866,-0.698448896408081,-0.6511330008506775,-0.7024165987968445,-0.6446875929832458,-0.6014852523803711,-0.5956829190254211,-0.618236780166626,-0.550102949142456,-0.5655697584152222,-0.530556321144104,-0.48344624042510986,-0.4604625403881073,-0.40844467282295227,-0.43316298723220825,-0.39455610513687134,-0.35314249992370605,-0.29603320360183716,-0.2983904182910919,-0.2529951333999634,-0.23611976206302643,-0.2902604639530182,-0.2316257804632187,-0.19511525332927704,-0.1654495745897293,-0.1811845749616623,-0.17578503489494324,-0.19721722602844238,-0.17229101061820984,-0.1136651337146759,-0.08196911215782166,-0.1391286551952362,-0.1120496317744255,-0.08190886676311493,-0.013630561530590057,-0.03790384531021118,-0.028804665431380272,-0.009207140654325485,-0.05520806089043617,-0.01300659030675888,0.025805197656154633,0.021985439583659172,0.028728369623422623,-0.010845839977264404,0.046721961349248886,0.0473642535507679,0.007268726825714111,0.04526880756020546,0.07535842061042786,-0.004649944603443146,0.0025128424167633057,-0.0007224720902740955,0.031032133847475052,0.05273479223251343,0.01972150430083275,0.0641157478094101,0.016785193234682083,0.025255218148231506,0.034046005457639694,-0.025459174066781998,-0.03824523091316223,0.0007103867828845978,-0.04169464483857155,-0.0012671463191509247,-0.0151893999427557,-0.029169609770178795,-0.11553994566202164,-0.07537102699279785,-0.1304740458726883,-0.07277536392211914,-0.15350846946239471,-0.15080222487449646,-0.13585668802261353,-0.19837936758995056,-0.153456449508667,-0.2442619502544403,-0.261513888835907,-0.21660494804382324,-0.2609128952026367,-0.25985997915267944,-0.34201350808143616,-0.29491570591926575,-0.3526250720024109,-0.4328736662864685,-0.4208845794200897,-0.48036623001098633,-0.5059851408004761,-0.4766940474510193,-0.4675024747848511,-0.5626363158226013,-0.5851514339447021,-0.6309744119644165,-0.6551798582077026,-0.6509602665901184,-0.6399431228637695,-0.6915420293807983,-0.6988009810447693,-0.7851949334144592,-0.7541434168815613,-0.8178569078445435,-0.8305196166038513,-0.8246271014213562,-0.8887775540351868,-0.9319011569023132,-0.9148935675621033,-0.9469547867774963,-1.0293021202087402,-1.058703899383545,-1.0831774473190308,-1.1161911487579346,-1.1276355981826782,-1.0886147022247314,-1.1634241342544556,-1.224717617034912,-1.1700108051300049,-1.2463164329528809,-1.2986390590667725,-1.2937123775482178,-1.2833317518234253,-1.3646016120910645,-1.3539602756500244,-1.4010761976242065,-1.3940645456314087,-1.3966503143310547,-1.4290940761566162,-1.4880512952804565,-1.4910248517990112,-1.4699795246124268,-1.5063389539718628,-1.5309033393859863,-1.5815502405166626,-1.5719587802886963,-1.5414719581604004,-1.6232175827026367,-1.624157428741455,-1.6436668634414673,-1.6564539670944214,-1.635105013847351,-1.625641107559204,-1.6176074743270874,-1.6207607984542847,-1.7124099731445312,-1.6664878129959106,-1.683419108390808,-1.7077354192733765,-1.7131390571594238,-1.700729250907898,-1.7608782052993774,-1.7584192752838135,-1.6697462797164917,-1.674918532371521,-1.6756138801574707,-1.7071701288223267,-1.6696020364761353,-1.7447459697723389,-1.74748694896698,-1.6677662134170532,-1.7032278776168823,-1.6496373414993286,-1.6850594282150269,-1.6930129528045654,-1.6583716869354248,-1.6708084344863892,-1.6125973463058472,-1.693190097808838,-1.6164367198944092,-1.6630173921585083,-1.5923733711242676,-1.5669441223144531,-1.538403034210205,-1.602791666984558,-1.5176770687103271,-1.515363335609436,-1.480303406715393,-1.5200371742248535,-1.4398683309555054,-1.4437212944030762,-1.4036363363265991,-1.4674289226531982,-1.4585318565368652,-1.384832739830017,-1.3467737436294556,-1.3755958080291748,-1.3522197008132935,-1.2504668235778809,-1.290520429611206,-1.22183096408844,-1.2541030645370483,-1.2163876295089722,-1.1590336561203003,-1.1205646991729736,-1.0991679430007935,-1.1244707107543945,-1.0709642171859741,-1.0836557149887085,-1.0192396640777588,-0.9970889687538147,-0.9710764288902283,-0.9329827427864075,-0.8701680898666382,-0.9068295359611511,-0.824981689453125,-0.8888251185417175,-0.8619612455368042,-0.8260127305984497,-0.7303493022918701,-0.7515376210212708,-0.7388173341751099,-0.7272127270698547,-0.6560331583023071,-0.6299351453781128,-0.6100401878356934,-0.5588238835334778,-0.514564573764801,-0.48507288098335266,-0.5361770391464233,-0.4683719277381897,-0.45016828179359436,-0.4068545699119568,-0.46004682779312134,-0.3987254798412323,-0.42118769884109497,-0.3877181112766266,-0.2879894971847534,-0.27447807788848877,-0.2529195547103882,-0.2915132939815521,-0.20719054341316223,-0.2537633776664734,-0.20802316069602966,-0.1868399828672409,-0.20508705079555511,-0.22183524072170258,-0.20290948450565338,-0.10320229083299637,-0.12427650392055511,-0.10702834278345108,-0.0801762193441391,-0.1216898262500763,-0.14071975648403168,-0.12234450876712799,-0.029618792235851288,-0.08691205084323883,-0.03661381080746651,-0.07674377411603928,-0.0824374333024025,-0.020509447902441025,-0.022861137986183167,-0.030983295291662216,-0.006149042397737503,-0.0970606803894043,-0.029426299035549164,-0.01259538158774376,-0.009970217011868954,-0.024298980832099915,-0.09197666496038437,-0.08669787645339966,-0.09239152818918228,-0.12039533257484436,-0.09387530386447906,-0.13629652559757233,-0.11269693821668625,-0.13518153131008148,-0.13646237552165985,-0.1598181426525116,-0.1368417888879776,-0.1224428117275238,-0.13102038204669952,-0.16359560191631317,-0.20095328986644745,-0.2512091398239136,-0.266966313123703,-0.2107878476381302,-0.25176694989204407,-0.29740190505981445,-0.3626473546028137,-0.379509299993515,-0.3611065447330475,-0.33173906803131104,-0.4036629796028137,-0.40829193592071533,-0.46326887607574463,-0.43997105956077576,-0.5115044116973877,-0.47879934310913086,-0.5437695980072021,-0.5453814268112183,-0.6184394955635071,-0.5900298357009888,-0.6203531622886658,-0.6413688659667969,-0.7405073642730713,-0.7187724709510803,-0.7107958197593689,-0.8290227055549622,-0.7627116441726685,-0.8432539701461792,-0.8448022603988647,-0.9299571514129639,-0.9445905089378357,-0.9613884091377258,-0.9554792642593384,-0.9925460815429688,-1.07651948928833,-1.0942472219467163,-1.113791584968567,-1.1339399814605713,-1.1789586544036865,-1.2131423950195312,-1.1933039426803589,-1.227629542350769,-1.2090994119644165,-1.239141583442688,-1.3524092435836792,-1.285262942314148,-1.30583918094635,-1.4132490158081055,-1.3743518590927124,-1.4585189819335938,-1.4069116115570068,-1.4825899600982666,-1.5314942598342896,-1.510082721710205,-1.5596193075180054,-1.5447691679000854,-1.5420539379119873,-1.5682109594345093,-1.5723340511322021,-1.6598875522613525,-1.6163866519927979,-1.6762977838516235,-1.658719539642334,-1.7228819131851196,-1.7206835746765137,-1.721314549446106,-1.7184535264968872,-1.7352651357650757,-1.758708119392395,-1.7965418100357056,-1.8135651350021362,-1.761025071144104,-1.82194185256958,-1.789306402206421,-1.8533284664154053,-1.7878005504608154,-1.8132809400558472,-1.8230020999908447,-1.806119441986084,-1.8277111053466797,-1.860982894897461,-1.8318419456481934,-1.8285753726959229,-1.8260395526885986,-1.8912625312805176,-1.8459980487823486,-1.8667497634887695,-1.8351898193359375,-1.8807512521743774,-1.8178822994232178,-1.7978575229644775,-1.802720069885254,-1.7936890125274658,-1.815798282623291,-1.7773076295852661,-1.7620993852615356,-1.7368642091751099,-1.790045142173767,-1.7342159748077393,-1.7216500043869019,-1.7537357807159424,-1.6836904287338257],\"z\":[0.041613806039094925,-0.05056740716099739,-0.032599810510873795,-0.08909891545772552,-0.007572472095489502,-0.0394606813788414,-0.0590977743268013,-0.06874386966228485,-0.08955316990613937,-0.07332894206047058,-0.11181709915399551,-0.12705211341381073,-0.18358449637889862,-0.18666808307170868,-0.24158890545368195,-0.22811825573444366,-0.235810324549675,-0.20653435587882996,-0.26085495948791504,-0.26494699716567993,-0.34111398458480835,-0.2951745092868805,-0.3886636793613434,-0.37508079409599304,-0.37898921966552734,-0.362903892993927,-0.450707346200943,-0.4733698070049286,-0.42614859342575073,-0.5332323312759399,-0.47544002532958984,-0.5679327249526978,-0.5924259424209595,-0.5370249152183533,-0.6355918645858765,-0.6077240109443665,-0.6025035977363586,-0.665726900100708,-0.7101396322250366,-0.7245895862579346,-0.7835285067558289,-0.7808059453964233,-0.7802290320396423,-0.8415241241455078,-0.8529242873191833,-0.9096875786781311,-0.938509464263916,-0.931057870388031,-0.9686208367347717,-1.0205137729644775,-0.9718009829521179,-1.0293244123458862,-1.062138319015503,-1.0592803955078125,-1.1194543838500977,-1.2047290802001953,-1.1896345615386963,-1.2701786756515503,-1.2810044288635254,-1.2916892766952515,-1.3591111898422241,-1.3031100034713745,-1.4001661539077759,-1.4162112474441528,-1.4748969078063965,-1.4290213584899902,-1.4808870553970337,-1.4961131811141968,-1.5162492990493774,-1.609316110610962,-1.630972981452942,-1.6383299827575684,-1.6749844551086426,-1.7167168855667114,-1.7251827716827393,-1.8036811351776123,-1.8444174528121948,-1.8352880477905273,-1.8983473777770996,-1.8649991750717163,-1.8931732177734375,-1.9925830364227295,-1.9657905101776123,-2.0304572582244873,-2.0887532234191895,-2.1357383728027344,-2.0779645442962646,-2.179884433746338,-2.1925430297851562,-2.184199810028076,-2.217017889022827,-2.226060628890991,-2.3355839252471924,-2.313399076461792,-2.3453402519226074,-2.402177333831787,-2.383308172225952,-2.4976139068603516,-2.531432628631592,-2.5532419681549072,-2.5546185970306396,-2.5570380687713623,-2.5647995471954346,-2.6625571250915527,-2.679215669631958,-2.736509084701538,-2.6977040767669678,-2.7084567546844482,-2.763960838317871,-2.789879322052002,-2.8165793418884277,-2.8320765495300293,-2.877101421356201,-2.920689582824707,-2.944732189178467,-2.965862989425659,-3.0101349353790283,-3.049896717071533,-3.0570249557495117,-3.0292980670928955,-3.0766687393188477,-3.1589853763580322,-3.1886813640594482,-3.1752994060516357,-3.21303653717041,-3.201010227203369,-3.2637860774993896,-3.284252643585205,-3.3191280364990234,-3.2683959007263184,-3.2944936752319336,-3.399590253829956,-3.4039247035980225,-3.4093284606933594,-3.439701557159424,-3.4038703441619873,-3.456399440765381,-3.5217502117156982,-3.476278305053711,-3.480067491531372,-3.569376230239868,-3.5837881565093994,-3.6108627319335938,-3.582120418548584,-3.6492490768432617,-3.601060390472412,-3.6116862297058105,-3.6672585010528564,-3.684340000152588,-3.6901824474334717,-3.7394022941589355,-3.6992387771606445,-3.715582847595215,-3.7011187076568604,-3.789121627807617,-3.735389232635498,-3.8171563148498535,-3.7891499996185303,-3.839358329772949,-3.85101056098938,-3.82366943359375,-3.870330333709717,-3.7928409576416016,-3.880768060684204,-3.8809750080108643,-3.829941511154175,-3.857297897338867,-3.832991361618042,-3.8518142700195312,-3.8506581783294678,-3.8935163021087646,-3.855743885040283,-3.886991024017334,-3.853306770324707,-3.8441646099090576,-3.9261538982391357,-3.870049238204956,-3.858660936355591,-3.9145963191986084,-3.879786729812622,-3.8911046981811523,-3.8919408321380615,-3.926887273788452,-3.853691339492798,-3.8748183250427246,-3.8543410301208496,-3.8843231201171875,-3.9060556888580322,-3.8816778659820557,-3.9187967777252197,-3.8989222049713135,-3.8454058170318604,-3.809849500656128,-3.8078176975250244,-3.87485408782959,-3.795057535171509,-3.864938735961914,-3.8061273097991943,-3.8264353275299072,-3.8293373584747314,-3.752793073654175,-3.817880630493164,-3.786435604095459,-3.764775037765503,-3.7613015174865723,-3.7638907432556152,-3.6837010383605957,-3.725278615951538,-3.7272000312805176,-3.7137577533721924,-3.712597370147705,-3.6113216876983643,-3.600893020629883,-3.623568058013916,-3.6184592247009277,-3.621901750564575,-3.5690972805023193,-3.5318853855133057,-3.5561940670013428,-3.5525760650634766,-3.536745071411133,-3.4442174434661865,-3.479538917541504,-3.444826126098633,-3.4099326133728027,-3.4149270057678223,-3.381925106048584,-3.3748905658721924,-3.334214687347412,-3.291689395904541,-3.2461581230163574,-3.3174331188201904,-3.2324273586273193,-3.184569835662842,-3.149777412414551,-3.207038164138794,-3.144763708114624,-3.080907106399536,-3.1052722930908203,-3.0336718559265137,-3.058856964111328,-3.0333805084228516,-2.99369215965271,-2.9617772102355957,-2.9748709201812744,-2.930056095123291,-2.915534734725952,-2.862492561340332,-2.8791184425354004,-2.81777024269104,-2.748626947402954,-2.7069900035858154,-2.7469732761383057,-2.6519317626953125,-2.6972599029541016,-2.5924363136291504,-2.634093999862671,-2.578657388687134,-2.581599473953247,-2.550980806350708,-2.4929044246673584,-2.472036361694336,-2.3892390727996826,-2.4124844074249268,-2.349602460861206,-2.3194339275360107,-2.341580629348755,-2.246333360671997,-2.248140811920166,-2.19732928276062,-2.1691949367523193,-2.159881114959717,-2.078754186630249,-2.0698091983795166,-2.057542324066162,-2.0550293922424316,-1.94419527053833,-1.911494255065918,-1.94987154006958,-1.880999207496643,-1.854626178741455,-1.843433141708374,-1.845824122428894,-1.7565948963165283,-1.776174545288086,-1.6736201047897339,-1.6638312339782715,-1.634326696395874,-1.605478048324585,-1.574553370475769,-1.5208518505096436,-1.5079545974731445,-1.509563684463501,-1.4870346784591675,-1.4021856784820557,-1.3760368824005127,-1.3373146057128906,-1.3073513507843018,-1.302709698677063,-1.3287378549575806,-1.2552186250686646,-1.2083362340927124,-1.20835542678833,-1.1858315467834473,-1.1598378419876099,-1.0853508710861206,-1.0598241090774536,-1.0425595045089722,-1.0199522972106934,-1.0193073749542236,-0.9479383826255798,-0.9839311242103577,-0.914247453212738,-0.9292424321174622,-0.8133575916290283,-0.8675967454910278,-0.8436337113380432,-0.804332971572876,-0.7862098217010498,-0.7606083154678345,-0.6863385438919067,-0.6378940343856812,-0.6284758448600769,-0.6586812734603882,-0.5988363027572632,-0.5296379923820496,-0.5879237055778503,-0.5141879320144653,-0.47501295804977417,-0.4555984139442444,-0.46998968720436096,-0.45367398858070374,-0.3750529885292053,-0.43719014525413513,-0.39718419313430786,-0.3776831328868866,-0.3376171886920929,-0.36014115810394287,-0.27218976616859436,-0.28709807991981506,-0.2751661539077759,-0.22745445370674133,-0.20954695343971252,-0.2220565676689148,-0.18287917971611023,-0.20636986196041107,-0.16862787306308746,-0.18106438219547272,-0.10297621041536331,-0.1346631944179535,-0.14934881031513214,-0.0791916623711586,-0.030648432672023773,-0.08427181839942932,-0.07420268654823303,-0.04363459721207619,-0.0564231351017952,-0.036456599831581116,0.028038181364536285,-0.012283649295568466,0.011780859902501106,0.016959819942712784,0.03283514082431793,0.02422960102558136,0.09179575741291046,0.07250703871250153,0.06489910930395126,0.020543891936540604,0.07853084802627563,0.075771763920784,0.1093507707118988,0.11946085095405579,0.05909387767314911,0.04429949074983597,0.06696952134370804,0.07566851377487183,0.1420811414718628,0.12889736890792847,0.11879009008407593,0.05593125522136688,0.1258428394794464,0.07823050022125244,0.1295906901359558,0.10280568897724152,0.09592556208372116,0.10873794555664062,0.1250058263540268,0.07744000107049942,0.09408936649560928,0.07806988060474396,0.05091262608766556,0.08815519511699677,0.052623599767684937,0.06616654247045517,0.004266951233148575,0.018229776993393898,0.051583461463451385,0.0474063903093338,0.021212304010987282,-0.026607198640704155,0.008117018267512321,-0.03760068118572235,-0.03430766612291336,-0.054242104291915894,-0.047963183373212814,-0.07324609160423279,-0.0566091388463974,-0.10058663040399551,-0.13867799937725067,-0.11073267459869385,-0.11968982219696045,-0.18394750356674194,-0.1681850701570511,-0.21518607437610626,-0.1457812339067459,-0.19451962411403656,-0.22438034415245056,-0.27108314633369446,-0.24356652796268463,-0.3277119994163513,-0.31282660365104675,-0.2943931519985199,-0.29945579171180725,-0.3340495824813843,-0.39754873514175415,-0.3680054545402527,-0.38707849383354187,-0.45685118436813354,-0.4555036127567291,-0.500411868095398,-0.5328431129455566,-0.5609803199768066,-0.5511486530303955,-0.5566460490226746,-0.6070573329925537,-0.6244968175888062,-0.6991442441940308,-0.6939699053764343,-0.6691697835922241,-0.6941311955451965,-0.7954885363578796,-0.7659377455711365,-0.7581536769866943,-0.8300734162330627,-0.8813647031784058,-0.8774515986442566,-0.8832228779792786,-0.9324829578399658,-0.9201161861419678,-0.9877049326896667,-1.0289814472198486,-1.032198190689087,-1.0318844318389893,-1.0916482210159302,-1.1817723512649536,-1.157904863357544,-1.2381327152252197,-1.185314416885376,-1.2552214860916138,-1.312802791595459,-1.2708253860473633,-1.3710449934005737,-1.3913049697875977,-1.3964359760284424,-1.449424386024475,-1.4971067905426025,-1.4906820058822632,-1.5293805599212646,-1.519326090812683,-1.6110539436340332,-1.617919683456421,-1.6406424045562744,-1.6740336418151855,-1.7506442070007324,-1.708409309387207,-1.7748912572860718,-1.834986686706543,-1.8885705471038818,-1.8442074060440063,-1.9389793872833252,-1.915117859840393,-1.9580129384994507,-2.021777391433716,-2.013439178466797,-2.025322198867798,-2.0910394191741943,-2.1222894191741943,-2.183283567428589,-2.146151304244995,-2.2588679790496826,-2.2561542987823486,-2.305412769317627,-2.2998359203338623,-2.294844150543213,-2.397587299346924,-2.385776996612549,-2.400862693786621,-2.5078840255737305,-2.5068793296813965,-2.5602095127105713],\"type\":\"scatter3d\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"scene\":{\"domain\":{\"x\":[0.0,1.0],\"y\":[0.0,1.0]},\"xaxis\":{\"title\":{\"text\":\"X\"}},\"yaxis\":{\"title\":{\"text\":\"Y\"}},\"zaxis\":{\"title\":{\"text\":\"Z\"}}},\"legend\":{\"title\":{\"text\":\"C\"},\"tracegroupgap\":0},\"margin\":{\"t\":60}},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('60ffe0a4-74d4-40cf-bfdb-da433579caa2');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "fig = px.line_3d(d, x=\"X\", y=\"Y\", z=\"Z\", color='C')\n",
        "# fig = px.scatter_3d(data, x=\"X\", y=\"Y\", z=\"Z\")\n",
        "fig.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}